[
{
	"uri": "//localhost:1313/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "//localhost:1313/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Tri Dung\nPhone Number: 032 986 2337\nEmail: nt.dung1297@gmail.com\nUniversity: FPT University HCMC\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 00/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "//localhost:1313/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "//localhost:1313/3-blogstranslated/3.1-blog1/",
	"title": "Unified multimodal access layer for Quora’s Poe using Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Authors: Gilbert V. Lepadatu and Nick Huber | Posted: 16/09/2025 | Categories: Amazon Bedrock, Customer Solutions, Foundation models, Generative AI\nOrganizations gain competitive advantage by deploying and integrating new generative AI models quickly through Generative AI Gateway architectures. This unified interface approach simplifies access to multiple foundation models (FMs), addressing a critical challenge: the proliferation of specialized AI models, each with unique capabilities, API specifications, and operational requirements. Rather than building and maintaining separate integration points for each model, the smart move is to build an abstraction layer that normalizes these differences behind a single, consistent API.\nThe AWS Generative AI Innovation Center and Quora recently collaborated on an innovative solution to address this challenge. Together, they developed a unified wrapper API framework that streamlines the deployment of Amazon Bedrock FMs on Quora’s Poe system. This architecture delivers a “build once, deploy multiple models” capability that significantly reduces deployment time and engineering effort, with real protocol bridging code visible throughout the codebase.\nFor technology leaders and developers working on AI multi-model deployment at scale, this framework demonstrates how thoughtful abstraction and protocol translation can accelerate innovation cycles while maintaining operational control.\nIn this post, we explore how the AWS Generative AI Innovation Center and Quora collaborated to build a unified wrapper API framework that dramatically accelerates the deployment of Amazon Bedrock FMs on Quora’s Poe system. We detail the technical architecture that bridges Poe’s event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, demonstrate how a template-based configuration system reduced deployment time from days to 15 minutes, and share implementation patterns for protocol translation, error handling, and multi-modal capabilities. We show how this “build once, deploy multiple models” approach helped Poe integrate over 30 Amazon Bedrock models across text, image, and video modalities while reducing code changes by up to 95%.\nQuora and Amazon Bedrock Poe.com is an AI system developed by Quora that users and developers can use to interact with a wide range of advanced AI models and assistants powered by multiple providers. The system offers multi-model access, enabling side-by-side conversations with various AI chatbots for tasks such as natural language understanding, content generation, image creation, and more.\nThe screenshot below showcases the user interface (UI) of Poe, the AI platform created by Quora. The image displays Poe’s extensive library of AI models, which are presented as individual “chatbots” that users can interact with.\nFigure 1: Poe’s AI model library interface\nThe following screenshot provides a view of the Model Catalog within Amazon Bedrock, a fully managed service from Amazon Web Services (AWS) that offers access to a diverse range of foundation models (FMs). This catalog acts as a central hub for developers to discover, evaluate, and access state-of-the-art AI from various providers.\nFigure 2: Model Catalog interface in Amazon Bedrock\nInitially, integrating the diverse FMs available through Amazon Bedrock presented significant technical challenges for the Poe.com team. The process required substantial engineering resources to establish connections with each model while maintaining consistent performance and reliability standards. Maintainability quickly emerged as an extremely important consideration, as did the ability to efficiently onboard new models as they became available—both factors adding further complexity to the integration challenges.\nTechnical challenge: Bridging different systems The integration between Poe and Amazon Bedrock presented fundamental architectural challenges that required innovative solutions. These systems were built with different design philosophies and communication patterns, creating a significant technical divide that the wrapper API needed to bridge.\nArchitectural divide The core challenge stems from the fundamentally different architectural approaches of the two systems. Understanding these differences is essential to appreciating the complexity of the integration solution.\nPoe operates on a modern, reactive, ServerSentEvents-based architecture through the FastAPI library (fastapi_poe). This architecture is stream-optimized for real-time interactions and uses an event-driven response model designed for continuous, conversational AI.\nAmazon Bedrock, on the other hand, functions as an enterprise cloud service. It offers REST-based APIs with AWS SDK access patterns, SigV4 authentication requirements, AWS Region-specific model availability, and a traditional request–response interaction pattern with streaming options.\nThis fundamental API mismatch creates several technical challenges that the Poe wrapper API solves, as summarized in the following table.\nTable: Integration challenges between Poe and Amazon Bedrock Challenge Category Technical Issue Source Protocol Target Protocol Integration Complexity Protocol Translation Converting between WebSocket-based protocol and REST APIs WebSocket (bidirectional, persistent) REST (request/response, stateless) High – requires protocol bridging Authentication Bridging Connecting JWT validation with AWS SigV4 signing JWT token validation AWS SigV4 authentication Medium – credential transformation needed Response Format Transformation Adapting JSON responses into expected format Standard JSON structure Custom format requirements Medium – data structure mapping Streaming Reconciliation Mapping chunked responses to ServerSentEvents Chunked HTTP responses ServerSentEvents stream High – real-time data flow conversion Parameter Standardization Creating unified parameter space across models Model-specific parameters Standardized parameter interface Medium – parameter normalization API evolution and the Converse API In May 2024, Amazon Bedrock introduced the Converse API, which offered standardization benefits that significantly simplified the integration architecture. Key capabilities include:\nA unified interface across diverse model providers (such as Anthropic, Meta, and Mistral). Conversation memory with consistent handling of chat history. Streaming and non-streaming modes through a single API pattern. Multimodal support for text, images, and structured data. Parameter normalization that reduces model-specific implementation quirks. Built-in content moderation capabilities. The solution presented in this post uses the Converse API where appropriate, while also maintaining compatibility with model-specific APIs for specialized capabilities. This hybrid approach provides flexibility while taking advantage of the Converse API’s standardization benefits.\nSolution overview The wrapper API framework provides a unified interface between Poe and Amazon Bedrock models. It serves as a translation layer that normalizes the differences between models and protocols while maintaining the unique capabilities of each model.\nThe solution architecture follows a modular design that separates concerns and enables flexible scaling, as illustrated in the following diagram.\nFigure 3: High-level architecture of the wrapper API framework between Poe and Amazon Bedrock\nThe wrapper API consists of several key components working together to provide a seamless integration experience:\nClient The entry point where users interact with AI capabilities through various interfaces (web browser, mobile app, APIs, and so on).\nPoe layer, which includes: Poe UI – Handles user experience, including request formation, parameter controls, file uploads, and response visualization. Poe FastAPI – Standardizes user interactions and manages the communication protocol between clients and underlying systems. This acts as a protocol management layer, ensuring consistency in how data and responses are transmitted. Bot Factory A factory pattern mechanism that dynamically creates appropriate model handlers (bots) based on the requested model type—for example, chat models, image generation models, or video generation models.\nThis structure makes it easy to extend the system to support new model types or variants of existing models in the future. The following code snippet illustrates how the Bot Factory selects the appropriate model handler based on the configuration:\n# From core/bot_factory.py - Actual implementation class BotFactory: \u0026#34;\u0026#34;\u0026#34; Factory for creating different types of bots. Handles bot creation based on the bot type and configuration. \u0026#34;\u0026#34;\u0026#34; @staticmethod def create_bot(bot_config: BotConfig) -\u0026gt; PoeBot: # Check if a custom bot class is specified if hasattr(bot_config, \u0026#39;bot_class\u0026#39;) and bot_config.bot_class: # Use the custom bot class directly bot = bot_config.bot_class(bot_config) # Explicitly ensure we\u0026#39;re returning a PoeBot if not isinstance(bot, PoeBot): raise TypeError(f\u0026#34;Custom bot class must return a PoeBot instance, got {type(bot)}\u0026#34;) return bot # Determine bot type based on configuration if hasattr(bot_config, \u0026#39;enable_video_generation\u0026#39;) and bot_config.enable_video_generation: # Video generation bot if \u0026#39;luma\u0026#39; in bot_config.bot_name: from core.refactored_luma_bot import LumaVideoBot return LumaVideoBot(bot_config) else: from core.refactored_nova_reel_bot import NovaReelVideoBot return NovaReelVideoBot(bot_config) elif hasattr(bot_config, \u0026#39;enable_image_generation\u0026#39;) and bot_config.enable_image_generation: # Image generation bot if hasattr(bot_config, \u0026#39;model_id\u0026#39;) and \u0026#34;stability\u0026#34; in bot_config.model_id.lower(): # Stability AI image generation bot from core.refactored_image_stability_ai import AmazonBedrockImageStabilityAIBot return AmazonBedrockImageStabilityAIBot(bot_config) else: # Other image generation bot (Titan, Canvas, etc.) from core.refactored_image_bot_amazon import RefactoredAmazonImageGenerationBot return RefactoredAmazonImageGenerationBot(bot_config) else: # Check if this is a Claude 3.7 model if hasattr(bot_config, \u0026#39;model_id\u0026#39;) and \u0026#34;claude-3-7\u0026#34; in bot_config.model_id.lower(): return ClaudePlusBot(bot_config) else: # Default to standard chat bot return RefactoredAmazonBedrockPoeBot(bot_config) Service manager Orchestrates the services needed to process requests effectively. It coordinates multiple specialized services, including:\nToken services – Managing token limits and counting. Streaming services – Handling real-time, streaming responses. Error services – Normalizing and handling errors consistently. AWS service integration – Managing API calls to Amazon Bedrock. AWS services component Converts responses from Amazon Bedrock format to Poe’s expected format and vice versa, handling streaming chunks, image data, and video outputs.\nAmazon Bedrock layer Amazon’s foundation model (FM) service that provides the actual AI processing capabilities and model hosting, including:\nModel diversity – Access to over 30 text models (such as Amazon Titan, Amazon Nova, Anthropic’s Claude, Meta’s Llama, Mistral, and others), as well as image and video models. API structure – Exposure of both model-specific APIs and the unified Converse API. Authentication – AWS SigV4 signing for secure access to model endpoints. Response management – Model outputs returned with standardized metadata and usage statistics. The request processing flow in this unified wrapper API illustrates the orchestration required when bridging Poe’s event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, showing how multiple specialized services collaborate to deliver a seamless user experience.\nThe flow begins when a client sends a request through Poe’s interface, which then forwards it to the Bot Factory component. This factory pattern dynamically creates the appropriate model handler based on the requested model type—chat, image, or video generation. The Service Manager then orchestrates the various specialized services required to process the request effectively, including token management, streaming, and error handling.\nThe sequence diagram below illustrates the complete request processing flow:\nConfiguration template for rapid multi-bot deployment The most powerful aspect of the wrapper API is its unified configuration template system, which supports rapid deployment and management of multiple bots with minimal code changes. This approach is central to the solution’s success in reducing deployment time.\nThe system uses a template-based configuration approach, combining shared defaults with model-specific overrides:\n# Bot configurations using the template pattern CHAT_BOTS = { \u0026#39;poe-nova-micro\u0026#39;: BotConfig( # Identity bot_name=\u0026#39;poe-nova-micro\u0026#39;, model_id=\u0026#39;amazon.nova-micro-v1:0\u0026#39;, aws_region=aws_config[\u0026#39;region\u0026#39;], poe_access_key=\u0026#39;XXXXXXXXXXXXXXXXXXXXXX\u0026#39;, # Model-specific parameters supports_system_messages=True, enable_image_comprehension=True, expand_text_attachments=True, streaming=True, max_tokens=1300, temperature=0.7, top_p=0.9, # Model-specific pricing enable_monetization=True, pricing_type=\u0026#34;variable\u0026#34;, input_token_cost_milli_cents=2, output_token_cost_milli_cents=4, image_analysis_cost_milli_cents=25, # Generate rate card with model-specific values custom_rate_card=create_rate_card(2, 4, 25), # Include common parameters **DEFAULT_CHAT_CONFIG ), \u0026#39;poe-mistral-pixtral\u0026#39;: BotConfig( # Identity bot_name=\u0026#39;poe-mistral-pixtral\u0026#39;, model_id=\u0026#39;us.mistral.pixtral-large-2502-v1:0\u0026#39;, aws_region=aws_config[\u0026#39;region\u0026#39;], poe_access_key=\u0026#39;XXXXXXXXXXXXXXXXXXXXXX\u0026#39;, # Model-specific parameters supports_system_messages=False, enable_image_comprehension=False, # ... # Include common parameters **DEFAULT_CHAT_CONFIG ) } This configuration-driven architecture offers several significant advantages:\nRapid deployment – Adding new models requires only creating a new configuration entry rather than writing integration code. This is a key factor in the significant improvement in deployment time. Consistent parameter management – Common parameters are defined once in DEFAULT_CHAT_CONFIG and inherited by bots, maintaining consistency and reducing configuration duplication. Model-specific customization – Each model can have its own unique settings while still benefiting from the shared infrastructure. Operational flexibility – Parameters can be adjusted without code changes, allowing for quick experimentation and optimization. Centralized credential management – AWS credentials are managed in one place, improving security and simplifying updates. Region-specific deployment – Models can be deployed to different AWS Regions as needed, with Region settings controlled at the configuration level. The BotConfig class provides a structured way to define bot configurations with type validation:\n# From config/bot_config.py - Actual implementation (partial) class BotConfig(BaseModel): # Core Bot Identity bot_name: str = Field(..., description=\u0026#34;Name of the bot\u0026#34;) model_id: str = Field(..., description=\u0026#34;Identifier for the AI model\u0026#34;) # AWS Configuration aws_region: Optional[str] = Field(default=\u0026#34;us-east-1\u0026#34;, description=\u0026#34;AWS region for deployment\u0026#34;) aws_access_key: Optional[str] = Field(default=None, description=\u0026#34;AWS access key\u0026#34;) aws_secret_key: Optional[str] = Field(default=None, description=\u0026#34;AWS secret key\u0026#34;) aws_security_token: Optional[str] = None # Poe Configuration poe_access_key: str = Field(..., description=\u0026#34;Poe access key\u0026#34;) modal_app_name: str = Field(..., description=\u0026#34;Modal app name\u0026#34;) # Capability Flags allow_attachments: bool = Field(default=True, description=\u0026#34;Whether to allow file attachments in Poe\u0026#34;) supports_system_messages: bool = Field(default=False) enable_image_comprehension: bool = Field(default=False) expand_text_attachments: bool = Field(default=False) streaming: bool = Field(default=False) enable_image_generation: bool = Field(default=False) enable_video_generation: bool = Field(default=False) # Inference Configuration max_tokens: Optional[int] = Field(default=None, description=\u0026#34;Maximum number of tokens to generate\u0026#34;) temperature: Optional[float] = Field(default=None, description=\u0026#34;Temperature for sampling\u0026#34;) top_p: Optional[float] = Field(default=None, description=\u0026#34;Top-p sampling parameter\u0026#34;) optimize_latency: bool = Field(default=False, description=\u0026#34;Enable latency optimization with performanceConfig\u0026#34;) # Reasoning Configuration (Claude 3.7+) enable_reasoning: bool = Field(default=False, description=\u0026#34;Enable Claude\u0026#39;s reasoning capability\u0026#34;) reasoning_budget: Optional[int] = Field(default=1024, description=\u0026#34;Token budget for reasoning (1024-4000 recommended)\u0026#34;) # Monetization Configuration enable_monetization: bool = Field(default=False, description=\u0026#34;Enable variable pricing monetization\u0026#34;) custom_rate_card: Optional[str] = Field( default=None, description=\u0026#34;Custom rate card for variable pricing in markdown format\u0026#34; ) input_token_cost_milli_cents: Optional[int] = Field( default=None, description=\u0026#34;Cost per input token in thousandths of a cent\u0026#34; ) output_token_cost_milli_cents: Optional[int] = Field( default=None, description=\u0026#34;Cost per output token in thousandths of a cent\u0026#34; ) image_analysis_cost_milli_cents: Optional[int] = Field( default=None, description=\u0026#34;Cost per image analysis in thousandths of a cent\u0026#34; ) Advanced multimodal capabilities One of the most powerful aspects of the framework is how it handles multimodal capabilities through simple configuration flags:\nenable_image_comprehension – When set to True for text-only models like Amazon Nova Micro, Poe itself uses vision capabilities to analyze images and convert them into text descriptions that are sent to the Amazon Bedrock model. This enables even text-only models to classify images without having built-in vision capabilities. expand_text_attachments – When set to True, Poe parses uploaded text files and includes their content in the conversation, enabling models to work with document content without requiring special file handling capabilities. supports_system_messages – This parameter controls whether the model can accept system prompts, allowing for consistent behavior across models with different capabilities. These configuration flags create a powerful abstraction layer that offers the following benefits:\nExtends model capabilities – Text-only models gain pseudo-multimodal capabilities through Poe’s preprocessing. Optimizes built-in features – True multimodal models can use their built-in capabilities for optimal results. Simplifies integration – Toggling capabilities is done via configuration switches instead of code changes. Maintains consistency – Provides a uniform user experience regardless of the underlying model’s native capabilities. Next, we explore the technical implementation of the solution in more detail.\nProtocol translation layer The most technically challenging aspect of the solution was bridging between Poe’s API protocols and the diverse model interfaces available through Amazon Bedrock. The team accomplished this through a sophisticated protocol translation layer:\n# From services/streaming_service.py - Actual implementation def _extract_content_from_event(self, event: Dict[str, Any]) -\u0026gt; Optional[str]: \u0026#34;\u0026#34;\u0026#34;Extract content from a streaming event based on model provider.\u0026#34;\u0026#34;\u0026#34; try: # Handle Anthropic Claude models if \u0026#34;message\u0026#34; in event: message = event.get(\u0026#34;message\u0026#34;, {}) if \u0026#34;content\u0026#34; in message and isinstance(message[\u0026#34;content\u0026#34;], list): for content_item in message[\u0026#34;content\u0026#34;]: if content_item.get(\u0026#34;type\u0026#34;) == \u0026#34;text\u0026#34;: return content_item.get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) elif \u0026#34;content\u0026#34; in message: return str(message.get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;)) # Handle Amazon Titan models if \u0026#34;delta\u0026#34; in event: delta = event.get(\u0026#34;delta\u0026#34;, {}) if \u0026#34;text\u0026#34; in delta: return delta.get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) # Handle other model formats if \u0026#34;chunk\u0026#34; in event: chunk_data = event.get(\u0026#34;chunk\u0026#34;, {}) if \u0026#34;bytes\u0026#34; in chunk_data: # Process binary data if present try: text = chunk_data[\u0026#34;bytes\u0026#34;].decode(\u0026#34;utf-8\u0026#34;) return json.loads(text).get(\u0026#34;completion\u0026#34;, \u0026#34;\u0026#34;) except Exception: self.logger.warning(\u0026#34;Failed to decode bytes in chunk\u0026#34;) # No matching format found return None This translation layer handles subtle differences between models and makes sure that regardless of which Amazon Bedrock model is being used, the response back to Poe is consistent and follows Poe’s expected format.\nError handling and normalization A critical aspect of the implementation is comprehensive error handling and normalization. The ErrorService provides consistent error handling across different models:\n# Simplified example of error handling (not actual code) class ErrorService: def normalize_Amazon_Bedrock_error(self, error: Exception) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Normalize Amazon Bedrock errors into a consistent format.\u0026#34;\u0026#34;\u0026#34; if isinstance(error, ClientError): if \u0026#34;ThrottlingException\u0026#34; in str(error): return \u0026#34;The model is currently experiencing high demand. Please try again in a moment.\u0026#34; elif \u0026#34;ValidationException\u0026#34; in str(error): return \u0026#34;There was an issue with the request parameters. Please try again with different settings.\u0026#34; elif \u0026#34;AccessDeniedException\u0026#34; in str(error): return \u0026#34;Access to this model is restricted. Please check your permissions.\u0026#34; else: return f\u0026#34;An error occurred while communicating with the model: {str(error)}\u0026#34; elif isinstance(error, ConnectionError): return \u0026#34;Connection error. Please check your network and try again.\u0026#34; else: return f\u0026#34;An unexpected error occurred: {str(error)}\u0026#34; This approach ensures that users receive meaningful error messages regardless of the underlying model or error condition.\nToken counting and optimization The system implements sophisticated token counting and optimization to maximize effective use of models:\n# From services/streaming_service.py - Actual implementation (partial) # Calculate approximate JSON overhead user_message_tokens = 0 for msg in conversation[\u0026#39;messages\u0026#39;]: for content_block in msg.get(\u0026#39;content\u0026#39;, []): if \u0026#39;text\u0026#39; in content_block: # Simple word-based estimation of actual text content user_message_tokens += len(content_block[\u0026#39;text\u0026#39;].split()) # Estimate JSON structure overhead (difference between total and content) json_overhead = int((input_tokens - system_tokens) - user_message_tokens) # Ensure we\u0026#39;re working with integers for calculations input_tokens_for_pct = int(input_tokens) system_tokens_for_pct = int(system_tokens) json_overhead_for_pct = int(json_overhead) # Calculate percentage with float arithmetic and proper integer division json_overhead_percent = (float(json_overhead_for_pct) / max(1, input_tokens_for_pct - system_tokens_for_pct)) * 100 ... This detailed token tracking enables accurate cost estimation and optimization, facilitating efficient use of model resources.\nAWS authentication and security The AwsClientService handles authentication and security for Amazon Bedrock API calls. This implementation provides secure authentication with AWS services while providing proper error handling and connection management.\nComparative analysis The implementation of the wrapper API dramatically improved the efficiency and capabilities of deploying Amazon Bedrock models on Poe, as summarized below.\nFeature Before (Direct API) After (Wrapper API) Deployment Time Days per model Minutes per model Developer Focus Configuration and plumbing Innovation and features Model Diversity Limited by integration capacity Extensive (across Amazon Bedrock models) Maintenance Overhead High (separate code for each model) Low (configuration-based) Error Handling Custom per model Standardized across models Cost Tracking Complex (multiple integrations) Simplified (centralized) Multimodal Support Fragmented Unified Security Varied implementations Consistent best practices This comparison highlights the significant improvements achieved through the wrapper API approach, demonstrating the value of investing in a robust abstraction layer.\nPerformance metrics and business impact The wrapper API framework delivered significant and measurable business impact across multiple dimensions, including increased model diversity, deployment efficiency, and developer productivity.\nPoe can rapidly expand its model offerings, integrating dozens of Amazon Bedrock models across text, image, and video modalities. This expansion occurred over a period of weeks rather than the months it would have taken with the previous approach.\nThe following table summarizes the deployment efficiency metrics:\nMetric Before After Improvement New Model Deployment 2–3 days 15 minutes 96× faster Code Changes Required 500+ lines 20–30 lines 95% reduction Testing Time 8–12 hours 30–60 minutes 87% reduction Deployment Steps 10–15 steps 3–5 steps 75% reduction These metrics were measured through direct comparison of engineering hours required before and after implementation, tracking actual deployments of new models.\nThe engineering team also observed a dramatic shift in focus from integration work to feature development, as shown in the table below:\nActivity Before (% of time) After (% of time) Change API Integration 65% 15% −50% Feature Development 20% 60% +40% Testing 10% 15% +5% Documentation 5% 10% +5% Scaling and performance considerations The wrapper API is designed to handle high-volume production workloads with robust scaling capabilities.\nConnection pooling To handle multiple concurrent requests efficiently, the wrapper implements connection pooling using aiobotocore. This allows it to maintain a pool of connections to Amazon Bedrock, reducing the overhead of establishing new connections for each request:\n# From services/aws_service.py - Connection management async def setup_client(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Initialize AWS client with proper configuration.\u0026#34;\u0026#34;\u0026#34; async with self._client_lock: try: # Always clean up existing clients first to avoid stale connections if self.Amazon_Bedrock_client: await self.cleanup() # Increase timeout for image generation config = Config( read_timeout=300, # 5 minutes timeout retries={\u0026#39;max_attempts\u0026#39;: 3, \u0026#39;mode\u0026#39;: \u0026#39;adaptive\u0026#39;}, connect_timeout=30 # 30 second connection timeout ) # Create the Amazon Bedrock client with proper error handling self.Amazon_Bedrock_client = await self.session.create_client( service_name=\u0026#34;Amazon_Bedrock-runtime\u0026#34;, region_name=self.bot_config.aws_region, aws_access_key_id=self.bot_config.aws_access_key, aws_secret_access_key=self.bot_config.aws_secret_key, aws_session_token=self.bot_config.aws_security_token, config=config ).__aenter__() except Exception as e: self.Amazon_Bedrock_client = None raise Asynchronous processing The entire framework uses asynchronous processing to handle concurrent requests efficiently:\n# From core/refactored_chat_bot.py - Asynchronous request handling async def get_response(self, query: QueryRequest) -\u0026gt; AsyncIterable[PartialResponse]: try: # Ensure AWS client is set up await aws_service.setup_client() # Validate and format the conversation conversation = await conversation_service.validate_conversation(query) # Process the request with streaming if self.bot_config.streaming: async for chunk in streaming_service.stream_Amazon_Bedrock_response(conversation, request_id): yield chunk else: # Non-streaming mode response_text, input_tokens, output_tokens = await streaming_service.non_stream_Amazon_Bedrock_response(conversation, request_id) if response_text: yield PartialResponse(text=response_text) else: yield PartialResponse(text=self.bot_config.fallback_response) # Send done event for non-streaming mode yield self.done_event() except Exception as e: # Error handling error_message = error_service.log_error(e, request_id, \u0026#34;Error during request processing\u0026#34;) yield PartialResponse(text=error_message) yield self.done_event() Error recovery and retry logic The system implements sophisticated error recovery and retry logic to handle transient issues:\n# From services/streaming_service.py - Retry logic max_retries = 3 base_delay = 1 # Start with 1 second delay for attempt in range(max_retries): try: if not self.aws_service.Amazon_Bedrock_client: yield PartialResponse(text=\u0026#34;Error: Amazon Bedrock client is not initialized\u0026#34;) break response = await self.aws_service.Amazon_Bedrock_client.converse_stream(**stream_config) # Process response... break # Success, exit retry loop except ClientError as e: if \u0026#34;ThrottlingException\u0026#34; in str(e): if attempt \u0026lt; max_retries - 1: delay = base_delay * (2 ** attempt) # Exponential backoff await asyncio.sleep(delay) continue error_message = f\u0026#34;Amazon Bedrock API Error: {str(e)}\u0026#34; yield PartialResponse(text=f\u0026#34;Error: {error_message}\u0026#34;) break Performance metrics The system collects detailed performance metrics to help identify bottlenecks and optimize performance:\n# From services/streaming_service.py - Performance metrics # Log token usage and latency latency = time.perf_counter() - start_time self.logger.info( f\u0026#34;[{request_id}] Streaming Response Metrics:\\n\u0026#34; f\u0026#34; Time to First Token: {first_token_time:.4f} seconds\\n\u0026#34; f\u0026#34; Input Tokens: {input_tokens} (includes system prompt)\\n\u0026#34; f\u0026#34; Input Tokens for Billing: {input_tokens - system_tokens} (excludes system prompt)\\n\u0026#34; f\u0026#34; Output Tokens: {output_tokens}\\n\u0026#34; f\u0026#34; Total Tokens: {total_tokens}\\n\u0026#34; f\u0026#34; Amazon Bedrock Latency: {latency:.4f} seconds\\n\u0026#34; f\u0026#34; Latency Optimization: {\u0026#39;enabled\u0026#39; if hasattr(self.bot_config, \u0026#39;optimize_latency\u0026#39;) and self.bot_config.optimize_latency else \u0026#39;disabled\u0026#39;}\u0026#34; ) Security considerations Security is a critical aspect of the wrapper implementation, with several key features to support secure operation.\nJWT validation with AWS SigV4 signing The system integrates JWT validation for Poe’s authentication with AWS SigV4 signing for Amazon Bedrock API calls:\nJWT validation – Ensures only authorized Poe requests can access the wrapper API. SigV4 signing – Ensures the wrapper API can securely authenticate with Amazon Bedrock. Credential management – AWS credentials are securely managed and not exposed to clients. Secrets management The system integrates with AWS Secrets Manager to securely store and retrieve sensitive credentials:\n# From services/aws_service.py - Secrets management @staticmethod def get_secret(secret_name: str, region_name: str = \u0026#34;us-east-1\u0026#34;) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34; Retrieve a secret from AWS Secrets Manager. Args: secret_name: Name of the secret to retrieve region_name: AWS region where the secret is stored Returns: Dict[str, Any]: The secret value as a dictionary \u0026#34;\u0026#34;\u0026#34; # Create a Secrets Manager client session = boto3.session.Session() client = session.client( service_name=\u0026#39;secretsmanager\u0026#39;, region_name=region_name ) try: get_secret_value_response = client.get_secret_value( SecretId=secret_name ) except Exception as e: logging.error(f\u0026#34;Error retrieving secret {secret_name}: {str(e)}\u0026#34;) raise # Depending on whether the secret is a string or binary, one of these fields will be populated. if \u0026#39;SecretString\u0026#39; in get_secret_value_response: import json try: # Explicitly annotate the return type for mypy result: Dict[str, Any] = json.loads(get_secret_value_response[\u0026#39;SecretString\u0026#39;]) return result except json.JSONDecodeError: # If not a JSON, return as a single-key dictionary return {\u0026#34;SecretString\u0026#34;: get_secret_value_response[\u0026#39;SecretString\u0026#39;]} else: import base64 decoded_binary_secret = base64.b64decode(get_secret_value_response[\u0026#39;SecretBinary\u0026#39;]) return {\u0026#34;SecretBinary\u0026#34;: decoded_binary_secret} Secure connection management The system implements secure connection management to help prevent credential leakage and ensure proper cleanup:\n# From services/aws_service.py - Secure connection cleanup async def cleanup(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Clean up AWS client resources.\u0026#34;\u0026#34;\u0026#34; try: if self.Amazon_Bedrock_client: try: await self.Amazon_Bedrock_client.__aexit__(None, None, None) except Exception as e: self.logger.error(f\u0026#34;Error closing Amazon Bedrock client: {str(e)}\u0026#34;) finally: self.Amazon_Bedrock_client = None self.logger.info(\u0026#34;Successfully cleaned up AWS client resources\u0026#34;) except Exception as e: # Even if cleanup fails, reset the references to avoid stale connections self.Amazon_Bedrock_client = None Troubleshooting and debugging The wrapper API includes comprehensive logging and debugging capabilities to help identify and resolve issues. The system implements detailed logging throughout the request processing flow. Each request is assigned a unique ID that is used throughout the processing flow to enable tracing:\n# From core/refactored_chat_bot.py - Request tracing request_id = str(id(query)) start_time = time.perf_counter() # Used in all log messages self.logger.info(f\u0026#34;[{request_id}] Incoming request received\u0026#34;) Lessons learned and best practices Through this collaboration, several important technical insights emerged that may benefit others undertaking similar projects:\nConfiguration-driven architecture – Using configuration files rather than hard-coded model-specific behaviors proved enormously beneficial for maintenance and extensibility. This approach allowed new models to be added without code changes, significantly reducing the risk of introducing bugs. Protocol translation challenges – The most complex aspect was handling the subtle differences in streaming protocols between different models. Building a robust abstraction required careful consideration of edge cases and comprehensive error handling. Error normalization – Creating a consistent error experience across diverse models required sophisticated error handling that could translate model-specific errors into user-friendly, actionable messages. This improved both developer and end-user experiences. Type safety – Strong typing (using Python’s type hints extensively) was crucial for maintaining code quality across a complex codebase with multiple contributors. This practice reduced bugs and improved code maintainability. Security first – Integrating Secrets Manager from the start ensured credentials were handled securely throughout the system’s lifecycle, helping prevent potential security vulnerabilities. Conclusion The collaboration between the AWS Generative AI Innovation Center and Quora demonstrates how thoughtful architectural design can dramatically accelerate AI deployment and innovation. By creating a unified wrapper API for Amazon Bedrock models, the teams were able to reduce deployment time from days to minutes while expanding model diversity and improving user experience.\nThis approach—focusing on abstraction, configuration-driven development, and robust error handling—offers valuable lessons for organizations looking to integrate multiple AI models efficiently. The patterns and techniques demonstrated in this solution can be applied to similar challenges across a wide range of AI integration scenarios.\nFor technology leaders and developers working on similar challenges, this case study highlights the value of investing in flexible integration frameworks rather than point-to-point integrations. The initial investment in building a robust abstraction layer pays dividends in long-term maintenance and capability expansion.\nTo learn more about implementing similar solutions, explore: AWS Well-Architected Framework – Best practices for building secure, high-performing, resilient, and efficient infrastructure. Amazon Bedrock Developer Guide – Detailed information on working with foundation models (FMs). AWS Generative AI Innovation Center – Support for innovative generative AI projects. AWS Prescriptive Guidance for LLM Deployment – Best practices for deploying large language models (LLMs). The AWS Generative AI Innovation Center and Quora teams continue to collaborate on enhancements to this framework, ensuring Poe users have access to the latest and most capable AI models with minimal deployment delay.\nAbout the authors Dr. Gilbert V. Lepadatu is a Senior Deep Learning Architect at the AWS Generative AI Innovation Center, where he helps enterprise customers design and deploy scalable, cutting-edge GenAI solutions. With a PhD in Philosophy and dual Master’s degrees, he brings a holistic and interdisciplinary approach to data science and AI. Nick Huber is the AI Ecosystem Lead for Poe (by Quora), where he is responsible for ensuring high-quality and timely integrations of leading AI models onto the Poe platform. "
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.1-event1/",
	"title": "Vietnam Cloud Day 2025",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025 Ho Chi Minh City Connect Edition for Builders – GenAI and Data Track Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nTime: 1PM Thursday, September 18, 2025\nEvent Objectives Provide an overview of the Agentic AI trend and AWS’s vision. Explore strategies for building a unified data foundation for AI \u0026amp; Analytics on AWS. Analyze the GenAI roadmap, AI Agent architectures, and challenges when moving to production. Learn about the AI-Driven Development Lifecycle (AI-DLC) model. Understand security principles, risk governance, and Responsible AI in Generative AI. Introduce new AWS services that support AI Agents and enhance enterprise productivity. Speaker List Jun Kai Loke – AI/ML Specialist Solutions Architect, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist Solutions Architect, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Christal Poon – Specialist Solutions Architect, AWS Key Highlights Opening – Agentic AI Overview – Jun Kai Loke Agentic AI and development trends Agentic AI is a leading strategic trend, aiming toward self-operating systems, reduced human supervision, and deep automation. Successful examples: Katalon, Apero, Techcom Securities. Amazon Bedrock – The AI development platform Secure deployment at scale Integration of tools and memory End-to-end monitoring Building a Unified Data Foundation on AWS – Kien Nguyen Current challenges 89% of CDOs are deploying GenAI, but only 52% consider their data platform ready (Harvard Business Review). Causes: data silos, people silos, business silos. End-to-End data strategy: Three main components:\nProducers Foundations Consumers Key AWS data components Amazon Bedrock Databases – RDS, specialized databases supporting vector search Analytics \u0026amp; ML – SageMaker, Unified Studio Data \u0026amp; AI Governance Lake House Architecture – S3, Redshift Managed Storage, Iceberg Open API Amazon DataZone GenAI Roadmap \u0026amp; AI Agents Architecture – Jun Kai Loke \u0026amp; Tamelly Lim Blueprint for building AI Agents: Model \u0026amp; application capabilities, tool framework.\nAmazon Bedrock\nAmazon Nova – new developments and customization\nStrands Agents – next-generation Agent models\nChallenges when bringing Agents to production\n→ AWS introduces Amazon Bedrock AgentCore to address these issues.\nAgentCore components Agent Core Runtime Agent Core Gateway Memory Agent Browser Code Interpreter → Enhances security and scalability.\nAI-Driven Development Lifecycle (AI-DLC) – Binh Tran Two current software development patterns AI Managed Pattern – low supervision but lower reliability AI Assisted Pattern – AI assists small tasks but still limited AI-DLC – The new software development lifecycle Includes 3 phases:\n1. Inception Build context Draft user stories Plan with work units 2. Construction Code + test Add architecture components Deploy IaC + testing 3. Operation Production deployment using IaC Incident management Securing Generative AI Applications – Taiki Dang Key security factors Compliance \u0026amp; Governance Legal \u0026amp; Privacy Controls Risk Management Resilience Scoping matrix Consumer App Enterprise App Pre-trained models Fine-tuned models Self-trained models Frameworks \u0026amp; Standards AWS Well-Architected MITRE ATLAS OWASP Top 10 for LLM Apps NIST AI 600-1 ISO 42001 EU AI Act Risks across layers Consumer: IP, Legal, Hallucination, Safety Tuner: managed/hosted, data retention Provider: training data, model construction Risk mitigation approaches Prompt engineering Fine-tuning RAG Parameter tuning Bedrock Guardrails Prompt security Beyond Automation: AI Agents as Productivity Multipliers – Christal Poon Types of AI Agent services Specialized Agents Fully-managed Agents DIY Agents Enterprise productivity services Amazon QuickSight\nAmazon Q:\nDashboards Reports Executive summaries AI Agent Scenarios Coming soon in Vietnam QuickSuite:\nQuick Researcher Quick Automate Humans in the Loop Key Learnings Understanding the Agentic AI ecosystem \u0026amp; AWS Roadmap: Agentic AI is the next generation of automation, oriented toward self-directed systems. Strong data architecture is the foundation of GenAI: S3, Iceberg, Redshift, Bedrock, SageMaker play central roles. AI-DLC enables a modern software development approach: Automation from planning → coding → testing → deployment. Security spans every layer of the AI stack: Requires compliance, data protection, and risk assessment. AWS is expanding the AI Agents \u0026amp; Enterprise AI ecosystem significantly. Applications to Work Integrating AI Agents into business workflows. Using Amazon Bedrock, Amazon Q, and Guardrails for quality and safety. Building a unified data foundation before GenAI. Applying the AI-DLC model to internal development. Building dashboards and insights with QuickSight \u0026amp; Amazon Q. Event Experience The workshop provided a clear view of the shift from traditional automation to Agentic AI.\nSpeakers delivered in-depth, practical content with clear direction for Vietnam’s GenAI adoption.\nThe combination of AgentCore, Bedrock, AI-DLC, and Amazon Q illustrates a comprehensive picture of enterprise AI.\nEvent Photos (Add your images here)\n"
},
{
	"uri": "//localhost:1313/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console and configure basic AWS services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 https://policies.fcjuni.com/ 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + Machine Learning and Artificial Intelligence + etc 09/08/2025 09/09/2025 https://docs.aws.amazon.com/whitepapers/latest/aws-overview/amazon-web-services-cloud-platform.html 4 - Create an AWS Free Tier account - Access Management with AWS Identity and Access Management (AWS IAM) - Manage usage costs on AWS with AWS Budgets - Practice: + Create AWS account + Create a User Group + Create new users + Add a MFA device + Create budget by Template + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Savings Plans Budget + Clean up Resources 09/09/2025 09/10/2025 https://000001.awsstudygroup.com/ https://000002.awsstudygroup.com/ https://000007.awsstudygroup.com/ 5 - Learn about Amazon Virtual Private Cloud (Amazon VPC) Security and Features: + Key capabilities + Network Access Control List (NACL)\n+ VPC Flow Logs + VPC Peering + Transit Gateway - Learn about VPN + VPN Site to Site + AWS Direct Connect - Learn about Elastic Load Balancing + Application Load Balancer + Network Load Balancer + Classic Load Balancer + Gateway Load Balancer - Practice + Create a VPC + Configure Site to Site VPN 09/10/2025 09/11/2025 https://000003.awsstudygroup.com/ 6 - Practice: + Create an EC2 instance + Create a Virtual Private Gateway + Create a Customer Gateway + Create a VPN connection + Configure Customer Gateway + Customize AWS VPN Tunnel + Configure advanced VPN settings 09/12/2025 09/12/2025 https://000003.awsstudygroup.com/ Week 1 Achievements: Understand what AWS is and grasp the basic service groups:\nCompute Storage Networking Database Machine Learning and Artificial Intelligence etc. Successfully created and configured an AWS Free Tier account.\nBecame familiar with AWS Management Console and learned how to search, access, and use basic services via the web interface.\nGot familiar with AWS IAM for access management.\nGot familiar with and began deploying network infrastructure with Amazon Virtual Private Cloud (AWS VPC).\n"
},
{
	"uri": "//localhost:1313/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Continue exploring basic AWS services, how to use the console, and customize basic AWS services.\nTasks to be implemented this week: Day Task Start Date Completion Date Reference Materials 2 - Start and deploy an application on Amazon Compute Cloud (Amazon Compute Cloud EC2) 09/15/2025 09/15/2025 https://000004.awsstudygroup.com/ 3 - Grant permissions for the application to access AWS services through IAM Role (AWS IAM) 09/16/2025 09/16/2025 https://000048.awsstudygroup.com/ 4 - Use Cloud IDE in the browser with (AWS Cloud9) 09/17/2025 09/17/2025 https://000049.awsstudygroup.com/ 5 - Host a static website with Amazon S3 09/18/2025 09/18/2025 https://000057.awsstudygroup.com/ 6 - Practice: + Create and set up an EC2 instance + Create users and user groups in AWS IAM + Create an S3 Bucket and host a static website 09/19/2025 09/19/2025 https://000004.awsstudygroup.com/ https://000048.awsstudygroup.com/ https://000057.awsstudygroup.com/ Week 2 Achievements: Successfully started and deployed an application on Amazon EC2, including basic instance configuration and connectivity verification.\nGranted appropriate permissions for the application to access required AWS services by creating and attaching IAM Roles, reinforcing understanding of role-based access control.\nBecame familiar with AWS Cloud9 as a cloud-based IDE, including source code editing, terminal usage, and basic integration with other AWS services.\nSet up and hosted a static website using Amazon S3, including bucket configuration, static website hosting settings, and public access configuration for website content.\nConsolidated practical skills by:\nCreating and configuring an EC2 instance. Creating IAM users and user groups and assigning suitable permissions. Creating an S3 bucket and successfully deploying a static website end-to-end. "
},
{
	"uri": "//localhost:1313/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Dưới đây là bản dịch tiếng Anh, giữ nguyên cấu trúc markdown và đã chỉnh sửa tất cả đường link để loại bỏ phần /vi/ đúng theo yêu cầu.\nWeek 3 Objectives: Continue learning how to use basic AWS services. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Materials 2 - Learn about Amazon Relational Database Service (Amazon RDS) 09/22/2025 09/22/2025 https://000005.awsstudygroup.com/ 3 - Optimize compute costs with Amazon Lightsail (Amazon Lightsail) 09/23/2025 09/23/2025 https://000045.awsstudygroup.com/ 4 - Learn about the mechanism of Amazon EC2 Auto Scaling 09/24/2025 09/24/2025 https://000006.awsstudygroup.com/ 5 - Create a system monitoring dashboard using Amazon CloudWatch 09/24/2025 09/25/2025 https://000008.awsstudygroup.com/ 6 - Practice: + Create a database with Amazon RDS + Create an EC2 instance with Auto Scaling + Configure Amazon CloudWatch to monitor the created EC2 instance 09/25/2025 09/26/2025 https://000005.awsstudygroup.com/ https://000006.awsstudygroup.com/ https://000008.awsstudygroup.com/ Week 3 Achievements: Gained understanding of the concepts, roles, and main use cases of Amazon RDS, including its advantages compared to managing a self-hosted database on EC2.\nUnderstood how to optimize compute costs with Amazon Lightsail, including its fixed-price plans and how it differs from EC2 in simpler deployment scenarios.\nLearned the basic operational mechanism of Amazon EC2 Auto Scaling, including:\nThe concept of an Auto Scaling Group. How EC2 instances automatically scale in or out based on system load. Learned how to create system monitoring dashboards using Amazon CloudWatch, including configuring metrics, monitoring charts, and tracking EC2 instance health.\nCompleted hands-on exercises:\nCreated a database on Amazon RDS and configured essential parameters. Created an EC2 instance in an Auto Scaling Group with automatic scaling policies. Configured Amazon CloudWatch to monitor the newly created EC2 instance, supporting performance tracking and operational visibility. "
},
{
	"uri": "//localhost:1313/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn how to migrate to AWS seamlessly and optimize systems on AWS. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Materials 2 - Learn about the AWS VM Import/Export service 09/29/2025 09/29/2025 https://000014.awsstudygroup.com/ 3 - Learn about AWS Database Migration Service (DMS) and AWS Schema Conversion Tool (SCT) 09/30/2025 09/30/2025 https://000043.awsstudygroup.com/ 4 - Get familiar with AWS Lambda, automate starting/stopping servers, and integrate Slack notifications 10/01/2025 10/01/2025 https://000022.awsstudygroup.com/ 5 - Create system monitoring dashboards using Amazon CloudWatch and Grafana 10/02/2025 10/02/2025 https://000029.awsstudygroup.com/ 6 - Manage resources in groups using Tag and Resource Groups - Manage access to EC2 services using IAM with Tag conditions - Manage services and automate operational tasks with AWS Systems Manager 15/08/2025 15/08/2025 https://000027.awsstudygroup.com/ https://000028.awsstudygroup.com/ https://000031.awsstudygroup.com/ Week 4 Achievements: Understood the purpose, architecture, and use cases of AWS VM Import/Export for seamless migration of virtual machines from on-premises or other environments to AWS.\nLearned the workflow and role of:\nAWS Database Migration Service (DMS) in migrating data between different database engines while minimizing downtime. AWS Schema Conversion Tool (SCT) in converting schemas during heterogeneous database migrations. Became familiar with AWS Lambda and the serverless execution model, including:\nImplementing automatic start/stop logic for EC2 servers using Lambda. Exploring Slack integration to send notifications during automated tasks. Learned how to create monitoring dashboards using:\nAmazon CloudWatch for metrics collection, logging, and alarm creation. Grafana for visualizing monitoring data from CloudWatch in a clear and intuitive manner. Gained understanding of how to manage resources using Tag and Resource Groups, including:\nApplying structured tags to resources for easier filtering, management, and cost allocation. Creating and using Resource Groups to work with sets of resources that share common attributes. Understood how to manage access to EC2 services using IAM with tag-based conditions, enabling:\nBuilding IAM policies based on Tags (tag-based access control). Restricting or granting permissions more granularly to specific resource groups. Became familiar with AWS Systems Manager and its role in:\nManaging configuration and operational state of resources. Automating operational tasks across EC2 instances and hybrid environments. "
},
{
	"uri": "//localhost:1313/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Continue learning how to optimize AWS systems. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Materials 2 - Implement a system backup plan using AWS Backup 10/06/2025 10/06/2025 https://000013.awsstudygroup.com/ 3 - Deploy an application using Docker - Deploy the application on Amazon Elastic Container Service (Amazon ECS) 10/07/2025 10/07/2025 https://000015.awsstudygroup.com/ https://000016.awsstudygroup.com/ 4 - Deploy the application with AWS CodePipeline - Automate application deployment using AWS CodePipeline 10/08/2025 10/08/2025 https://000017.awsstudygroup.com/ https://000023.awsstudygroup.com/ 5 - Learn how to configure Single Sign-On (Amazon SSO) 10/09/2025 10/09/2025 https://000012.awsstudygroup.com/ 6 - Learn about IAM Permission Boundary 10/10/2025 10/10/2025 https://000030.awsstudygroup.com/ Week 5 Achievements: Understood the role and overall architecture of AWS Backup, including how to define backup plans, configure backup vaults, and schedule automated backups for resources.\nUnderstood the workflow of packaging applications with Docker at a fundamental level.\nBecame familiar with Amazon ECS for container deployment:\nUnderstood the concepts of Task Definition, Service, and Cluster. Successfully deployed a containerized application to ECS following the documentation. Learned the process of application deployment using AWS CodePipeline, including:\nUnderstanding the main stages: Source – Build – Deploy. Setting up a CI/CD pipeline to automate deployment so that application updates are automatically built and deployed when source code changes. Explored the mechanism and benefits of AWS Single Sign-On (SSO):\nUnderstood the goal of centralized login management for multiple accounts and applications. Understood user/group authorization flows within the SSO system. Understood the concept of IAM Permission Boundary:\nDistinguished between a Permission Boundary and a standard IAM Policy. Learned how Permission Boundaries limit the maximum permissions that a user or role can receive, enabling stronger access control. "
},
{
	"uri": "//localhost:1313/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn how to modernize applications, get familiar with building Serverless architectures, and explore AI services on AWS. Collaborate with team members, brainstorm initial ideas, set up foundational services for the project, and explore additional related knowledge. Tasks to be implemented this week: Day Task Start Date Completion Date Reference Materials 2 - Serverless: Lambda interacting with S3 and DynamoDB 10/13/2025 10/13/2025 https://000078.awsstudygroup.com/ 3 - Call APIs through API Gateway 10/14/2025 10/14/2025 https://000079.awsstudygroup.com/ 4 - Learn about Amazon Cognito 10/15/2025 10/15/2025 https://000081.awsstudygroup.com/ 5 - Learn how to deploy applications using AWS Serverless Application Model (SAM) 10/16/2025 10/16/2025 https://000080.awsstudygroup.com/ 6 - Practice: + Deploy the Frontend + Deploy Lambda Functions + Configure API Gateway + Test APIs using Postman and the Frontend 10/17/2025 10/17/2025 https://000078.awsstudygroup.com/ https://000079.awsstudygroup.com/ https://000080.awsstudygroup.com/ Week 6 Achievements: Understood the Serverless model on AWS through hands-on labs with Lambda interacting with S3 and DynamoDB, including:\nHow Lambda reads/writes data from S3. How Lambda performs basic operations on DynamoDB. Understood the role of Amazon API Gateway in a serverless architecture:\nThe request flow from client → API Gateway → Lambda. How to configure endpoints to forward requests to Lambda functions. Explored an overview of Amazon Cognito:\nUnderstood User Pools and Identity Pools. Learned the purpose of Cognito in user authentication and user management for web/mobile applications. Became familiar with deploying applications using the AWS Serverless Application Model (AWS SAM):\nUnderstood the meaning of the template file (Infrastructure as Code – IaC). Learned the workflow of building, deploying, and updating serverless applications via SAM. Completed the end-to-end application deployment exercise, including:\nDeploying the Frontend that connects to the serverless backend. Deploying Lambda Functions to handle backend logic. Configuring API Gateway to connect the frontend with Lambda. Testing API functionality through Postman and validating results from the frontend interface. Coordinated with team members to:\nFinalize preliminary project ideas using a serverless architecture. Identify and deploy essential AWS services and strengthen related knowledge for the next development phase. "
},
{
	"uri": "//localhost:1313/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn about the architecture and basic concepts of Kubernetes. Learn how to deploy and manage applications on Amazon EKS. Build a complete CI/CD pipeline to automate deployment to EKS. Tasks accomplished this week: Day Task Start Date Completion Date Reference Material 2 Refine the architecture details.\nRead lab 126:\n- How to initialize a cluster, worker nodes \u0026amp; management architecture. - Understand basic concepts: Pod, Deployment, DaemonSet, … — how Kubernetes organizes containers. - How to deploy micro-services on EKS. 20/10/2025 21/10/2025 https://000126.awsstudygroup.com/vi/ 4 Project team meeting:\n- Agree on data sources and data formats.\n- Test features, fix bugs.\n- Define user flow. 22/10/2025 22/10/2025 5 Complete lab 62:\n- Set up Amazon EKS — initialize a Kubernetes cluster.\n- Use AWS CodePipeline + AWS CodeBuild + GitHub to create a CI/CD pipeline: whenever there is a new commit, the pipeline automatically builds the container, pushes the image, and deploys to EKS.\n- Configure RBAC to grant CodeBuild/CodePipeline permissions to deploy to the cluster. 23/10/2025 23/10/2025 https://000062.awsstudygroup.com/vi/ 6 Weekly team progress report meeting. 24/10/2025 24/10/2025 7 Prepare mid-term test materials. 24/10/2025 25/10/2025 Week 7 Achievements: In-depth Kubernetes Knowledge: Mastered the core concepts of Kubernetes and the architecture of Amazon EKS through theoretical study (Lab 126). CI/CD on EKS Skills: Successfully built a complete CI/CD pipeline, automating the entire process from building, pushing images to deploying applications on EKS, integrated with GitHub, CodePipeline, and CodeBuild (Lab 62). Project: Agreed on important details regarding data and user flow, while also conducting testing and bug fixing for developed features. Preparation for Mid-term test: Began organizing knowledge and preparing materials for the mid-term test. "
},
{
	"uri": "//localhost:1313/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Advanced access control and permission techniques in IAM. Learn to automate infrastructure using AWS CloudFormation. Review and complete the mid-term exam. Identify and resolve outstanding issues in the project. Tasks accomplished this week: Day Task Start Date Completion Date Reference Material 2 Complete lab 28:\n- Use tags + IAM to control EC2 operation permissions.\n- Create a conditional IAM policy based on tags to ensure “least privilege”.\n- Only users with a matching tag can perform actions on EC2.\nComplete lab 37: - Create an IAM User/Role to use CloudFormation.\n- Deploy a template, create a stack automatically.\nContinue preparing review materials. 27/10/2025 28/10/2025 https://000028.awsstudygroup.com/\nhttps://000037.awsstudygroup.com/ 3 Mid-term review. 29/10/2025 30/10/2025 6 Mid-term exam. 31/10/2025 31/10/2025 Sun Project team meeting: Identify tool errors and data issues, agree on solutions. 02/11/2025 02/11/2025 Week 8 Achievements: Advanced Security \u0026amp; Permissions: Learnt the use of tags for granular access control to resources, ensuring the principle of “least privilege” (Lab 28). Infrastructure as Code (IaC): Gained the ability to automate infrastructure creation and management using AWS CloudFormation (Lab 37). Completed Mid-term Goal: Thoroughly reviewed and completed the mid-term exam. Project Problem Solving: Proactively identified bugs and data-related issues in the project and agreed on a unified solution. "
},
{
	"uri": "//localhost:1313/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Refine the Proposal and finalize feature requirements that utilize Lambda. Review and align on the project\u0026rsquo;s branding identity. Explore methods to optimize costs for the proposed architecture. Continue processing and preparing data for the project. Tasks completed during the week: Day Task Start Date Completion Date Reference Material 2 Revised the Proposal according to the updated architecture. 03/11/2025 05/11/2025 5 Drafted the list of Lambda-based features. Developed Lambda functions. 06/11/2025 06/11/2025 6 Project team meeting:\n- Reviewed the data tool’s functionality.\n- Finalized key website features and AWS services to be used.\n- Team meeting to report progress. 07/11/2025 07/11/2025 7 Continued data collection.\nExplored cost-reduction methods.\nUpdated the architecture diagram to improve cost efficiency. 08/11/2025 09/11/2025 Week 9 Achievements: Proposal \u0026amp; AI Feature Requirements Completed: Refined the Proposal based on the new architecture and built Lambda functions to validate initial workflows. Feature \u0026amp; Service Alignment: Reached consensus on key features and AWS services to be used in the project. Cost Optimization Research: Actively explored cost-saving solutions and refined the architecture to reduce operational expenses. Data Processing: Continued data preparation to ensure reliable input for upcoming project features. "
},
{
	"uri": "//localhost:1313/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and core AWS services\nWeek 2: Learning IAM, S3, and deploying a static website\nWeek 3: Designing VPC, subnets, security groups, and launching basic EC2\nWeek 4: Working with RDS, configuring connections, and backing up data\nWeek 5: Setting up CloudWatch logs, alarms, and system monitoring dashboards\nWeek 6: Studying Auto Scaling, Load Balancer, and high availability architecture\nWeek 7: Hands-on workshop: accessing S3 from VPC via Gateway Endpoint\nWeek 8: Simulating on-premises and accessing S3 via Interface Endpoint\nWeek 9: Designing and applying VPC Endpoint Policies for secure S3 access\nWeek 10: Cleaning up resources, reviewing, and optimizing AWS costs\nWeek 11: Consolidating results, finalizing proposal, and internship report documents\nWeek 12: Finalizing portfolio, self-evaluation, receiving feedback, and preparing presentation\n"
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.2-event2/",
	"title": "AI-Driven Development Life Cycle",
	"tags": [],
	"description": "",
	"content": "Reimagining Software Engineering Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nTime: 2PM – 4:30PM Friday, October 03, 2025\nEvent Objectives Explore the transformation of the Software Development Lifecycle (SDLC) in the GenAI era.\nIntroduce the AI-Driven Development Lifecycle (AI-DLC) model and how to apply it in real-world projects.\nDemonstrate Amazon Q Developer and KIRO – the AI IDE that supports development from prototype to production.\nShare best practices for developers and businesses to maintain quality control when applying AI across the entire software development process.\nSpeaker List Toan Huynh – Senior Specialist Solutions Architect, AWS\nMy Nguyen – Senior Prototyping Architect, AWS\nKey Highlights 1. AI-Driven Development Life Cycle \u0026amp; Amazon Q Developer – Toan Huynh The evolution of AI and its impact on SDLC Evolution of AI: auto-complete → assistant → agents.\nEach stage reshapes how developers learn, write code, test, deploy, and operate software.\nEven with deep AI assistance, developers must remain the owners of the product: making decisions, approving quality, and taking professional responsibility.\nTwo approaches developers use when working with AI AI-assisted: AI is used for narrow tasks → still limited, not enough to cover the whole lifecycle.\nAI-managed: AI participates across the entire process, coordinating multiple agents with different roles → the future model.\n7 problems when using AI → the birth of AI-DLC AI-DLC lies between AI-Assisted and AI-Managed.\nIn AI-DLC, AI supports more tasks:\nPlanning\nArchitecture suggestions\nClarifying requirements\nGenerating scaffolding\nRefactoring\nTesting flows\nHowever: developers must still ensure:\nExpertise\nDecisions\nJudgment\nValidation\nCore Concepts of AI-DLC Mob development:\nMob Elaboration\nMob Construction\nSpec-driven development: effective but difficult to customize for complex problems.\nEach stage must be clear about context – input – output.\nWhen working with AI, always request process logs (not just final output).\nProvide clear instructions to AI:\nState exactly what you need\nProvide relevant documents\nAvoid prompts like “don’t do ABC”\nAI works best on tasks requiring precision and structured logic → leverage it for those areas.\nKey notes from the speaker Always ask AI to create a plan, then review–refine–repeat continuously.\nBreak down tasks before assigning them to AI.\nCreate separate sessions for each task to avoid context interference.\n2. KIRO – AI IDE for Prototype to Production – My Nguyen Introduction to KIRO An AI IDE designed for the spec → prototype → production workflow.\nSupports spec-driven development directly inside the IDE.\nIntuitive UI, easy to visualize the entire project workflow.\nKey features Agent hooks\nAdvanced context management\nWorkflow tracking\nOptimized for smaller projects (less documentation than traditional AI-DLC)\nIntegrating AI-DLC into KIRO AI-DLC methodology can be applied by:\nCreating a steering folder\nAdding AI-DLC workflows (markdown files) or company rules\nCompatible with VSCode, Claude models, and various modern technologies.\nCommon ground between the two approaches Both AI-DLC and KIRO are based on domain-driven design (DDD) to ensure clarity of boundaries, context, and business logic.\nKey Learnings AI-driven design mindset AI can automate most SDLC tasks, but humans remain the ultimate decision-makers.\nA clear workflow is required: context → inputs → outputs → validation.\nTechnical workflow AI-DLC helps automate from planning to testing.\nMob development + spec-driven approaches enhance transparency and quality control.\nAlways store AI reasoning logs for verification.\nEffective ways to work with AI Ask AI to generate a detailed plan first.\nBreak down problems.\nAvoid overloading a single session.\nLeverage AI’s strengths in structured and consistency-required tasks.\nTools Amazon Q Developer supports end-to-end SDLC automation.\nKIRO is suitable for smaller products and fast prototype-to-production needs.\nApplications to Work Apply AI-DLC to the current workflow to reduce planning, design, and review time.\nUse Amazon Q Developer to accelerate coding, architecture, and testing.\nUse KIRO for projects requiring rapid prototyping or high visual clarity.\nEstablish clear workflows + record reasoning to ensure quality control when AI has deep involvement in SDLC.\nEvent Experience The session provided a clear perspective on how GenAI is reshaping software engineering.\nThe speakers offered practical methodologies, modern tools, and applicable approaches for projects of any scale.\nThe combination of Amazon Q Developer and KIRO delivers a complete picture of AI-driven development – from enterprise workflows to prototype builds.\nSome photos from the event "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "//localhost:1313/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AI Contract Intelligence Platform - AGREEME A Serverless AWS Solution for Personal Contract Review TEEJ - AGREEME 1. Executive Summary The AI Contract Intelligence Platform is a web-based service for individuals and small user groups (freelancers, small business owners, administrative/legal staff) who work with contracts daily but lack deep legal expertise. The solution uses Amazon Bedrock and a fully serverless AWS architecture to analyze contracts, highlight risks, suggest clause edits, and generate summaries and new contract templates.\nBuilt on AWS Amplify, Lambda, API Gateway, DynamoDB, S3, Cognito, EventBridge, and CloudWatch, the platform delivers low-latency, low-cost, and secure AI-assisted contract review, optimized for single users or small teams without complex enterprise features.\n2. Problem Statement What’s the Problem? Contracts are long, complex, and difficult to understand for non-lawyers. Hiring legal consultants for every contract is expensive and not scalable for individuals. There is no simple, self-service tool focused on fast, accurate contract review for personal or small-business use. Users do not want complex multi-tenant systems or heavy document management; they just want quick risk checks and clear guidance. The Solution The platform provides an AI-powered web app where users can upload contract files (PDF/DOCX) and receive:\nPlain-language explanations of complex clauses. Legal context with highlighted favorable/unfavorable terms. Risk detection and alerts (unbalanced clauses, hidden obligations, potential legal issues). Clause-level suggestions and alternative wording for negotiation. Automatic executive summaries for busy users. Simple contract generation from templates (lease, sale, service, etc.), with AI-guided adjustments for real-world scenarios. All of this runs on a serverless AWS architecture:\nFrontend on AWS Amplify with integrated Hosting, CDN, and WAF. APIs \u0026amp; compute via Amazon API Gateway and AWS Lambda. AI via Amazon Bedrock (GenAI/LLM + embeddings/RAG). Storage \u0026amp; metadata via Amazon S3 and DynamoDB, encrypted by KMS. Identity \u0026amp; security via Amazon Cognito, AWS WAF, IAM, and KMS. Monitoring \u0026amp; events via Amazon CloudWatch and EventBridge. Benefits and Return on Investment Business impact\nReduce contract reading/understanding time by ≥ 70%. Reduce legal advisory costs by ≥ 50% by replacing initial lawyer review with AI. Increase user confidence when signing and negotiating contracts. Technical performance\nContract analysis accuracy ≥ 85% (internal tests + user feedback). Response time ≤ 5 seconds after upload. System uptime ≥ 99.9% for individual users. Cost efficiency\nEstimated AWS infra cost: $27.90/month → $334.80/12 months. Implementation effort: 592 hours total, ≈ $637.12 in team cost (Solution Architect + Software Engineer + AI Engineer). 3. Solution Architecture The platform is implemented as a fully serverless, secure, and scalable architecture optimized for GenAI-driven document processing and RAG-based contract intelligence. High-Level Architecture Entry \u0026amp; Web Layer\nAmazon Route 53 for DNS and friendly domain names. AWS Amplify Hosting for the React/Amplify-based frontend with integrated CDN and WAF. Identity \u0026amp; Access\nAmazon Cognito for user pools and authentication (JWT). API Gateway validates Cognito tokens before invoking Lambda. IAM roles enforce least-privilege access to S3, DynamoDB, Bedrock, KMS, and EventBridge. Backend Compute (Lambda Microservices)\nCore API Lambda for orchestrating requests from API Gateway.\nSpecialized Lambdas for:\nContract generation (ContractGen). General LLM calls (summaries, classification, transformations). RAG search (embedding-based retrieval and knowledge lookup). Metadata updates (DynamoDB). Template management. AI \u0026amp; LLM Layer\nAmazon Bedrock for:\nContract analysis (summary, risk, clause classification). Embeddings and RAG over legal corpus and templates. Contract generation and clause rewrite suggestions. Data \u0026amp; Storage\nAmazon S3 for user-uploaded contracts, generated documents, and templates. Amazon DynamoDB for metadata, templates, and RAG indices. AWS KMS for encryption at rest across S3, DynamoDB, and secrets. Events \u0026amp; Automation\nAmazon EventBridge for asynchronous workflows (background processing, metadata updates, template sync). Monitoring \u0026amp; Operations\nAmazon CloudWatch for logs, metrics, and alarms across Lambda, API Gateway, Amplify, and Bedrock interactions. AWS Services Used Application Stack\nAWS Amplify (frontend hosting, CI/CD, WAF integration). React / Amplify Framework (UI). Amazon API Gateway (API management). AWS Lambda (backend microservices). Amazon Bedrock (LLM inference, embeddings, RAG). Amazon DynamoDB (metadata and knowledge store). Amazon S3 (document storage). AWS KMS (encryption). Amazon EventBridge (event routing). Monitoring \u0026amp; DevOps\nAmazon CloudWatch (observability, alarms). GitLab + Amplify CI/CD (source control and automated deployment). Security\nAWS WAF (via Amplify). AWS IAM (access control). Amazon Cognito (authentication). Component Design Web Interface: Amplify-hosted React app for upload, analysis view, history, and contract generation. API Layer: API Gateway + Lambda for upload handling, Bedrock orchestration, and result retrieval. AI Logic: Bedrock-powered flows for clause analysis, risk scoring, summarization, and template-based generation. Data Layer: S3 for raw/generated contracts; DynamoDB for metadata, templates, and RAG indices. Security \u0026amp; Compliance: Cognito-based auth, KMS encryption, WAF protection, and strict IAM policies. 4. Technical Implementation Implementation Phases Phase 1 – Assessment (Week 1–2)\nGather business and user requirements. Define AI use cases (analysis, risk detection, suggestions). Design high-level architecture and security baseline. Phase 2 – Setup Base Infrastructure (Week 3–4)\nConfigure Amplify project, IAM roles, S3, DynamoDB, KMS. Enable CloudWatch logging and monitoring. Phase 3 – Frontend \u0026amp; Authentication (Week 5–6)\nDeploy frontend via Amplify. Integrate Cognito and Amplify-managed WAF. Implement contract upload and basic dashboard UI. Phase 4 – Backend Core \u0026amp; AI Integration (Week 7–8)\nImplement Lambda APIs and Bedrock integration. Parse contracts, store results in DynamoDB, and return analysis to UI. Phase 5 – Advanced AI Logic \u0026amp; Optimization (Week 9–10)\nImplement risk detection, negotiation suggestions, and RAG search. Improve UX, optimize latency and cost. Phase 6 – Testing, Go-Live \u0026amp; Handover (Week 11–12)\nUnit/integration tests, security and performance validation. Production deployment and knowledge transfer. Technical Requirements Proficiency with AWS Amplify, Lambda, API Gateway, Cognito, S3, DynamoDB, EventBridge, CloudWatch. Access to Amazon Bedrock (Claude, Llama, Titan, etc.) for legal-text analysis. File processing libraries for PDF/DOCX extraction. Clear data-handling and privacy policies for sensitive contract data. 5. Timeline \u0026amp; Milestones Total duration: 12 weeks (6 two-week sprints).\nSprints:\nSprint 1–2: Assessment \u0026amp; base infrastructure. Sprint 3–4: Frontend + authentication. Sprint 5–6: Backend core, AI integration, advanced AI logic, and optimization. Continuous Agile delivery with sprint planning, review, and retrospective.\nKnowledge transfer in final sprints (architecture, operations, CloudWatch, prompt best practices).\n6. Budget Estimation Infrastructure Costs (per month) Infrastructure Costs Service Monthly Cost (USD) 12-Month Cost (USD) Amazon S3 $1.80 $21.60 Amazon API Gateway $0.05 $0.60 Amazon DynamoDB $4.02 $48.24 AWS Secrets Manager $1.08 $12.96 Amazon Route 53 $2.04 $24.48 Amazon Cognito $1.00 $12.00 AWS Amplify $16.25 $195.00 Amazon CloudWatch $0.53 $6.36 Amazon Bedrock $1.13 $13.56 Total $27.90/month $334.80/12 months Implementation Team Cost Role Hourly Rate (USD) Solution Architect $2.30/hour Software Engineer $0.70/hour AI Engineer $0.70/hour Total estimated project effort: 592 hours → ≈ $637.12 USD.\n7. Risk Assessment Key Risks AI accuracy risk: Misinterpretation of certain legal clauses. File quality risk: Low-quality scans or complex PDFs may break OCR/parsing. Sensitive data risk: Users may upload highly confidential contracts. Cloud dependency risk: Outages in Bedrock or Amplify affect availability. Usage \u0026amp; cost risk: High volume of AI calls increases operating cost. Mitigation Strategies Thorough internal testing and clear disclaimers on AI limitations. Robust file-processing pipeline with validation and user guidance. Strong encryption (KMS), limited retention, and strict access controls. Monitoring and alerts through CloudWatch, with incident runbooks. Prompt optimization, request limits, and budget alerts to control AI cost. 8. Expected Outcomes Technical Outcomes Production-ready, serverless AI contract assistant for individuals/small teams. Stable performance with ≤ 5s analysis time and ≥ 99.9% uptime. Secure handling of sensitive documents with encryption and least-privilege access. Business Outcomes Faster contract understanding and reduced legal risk for non-expert users. Significant reduction in legal advisory costs and manual review time. A scalable SaaS foundation that can be extended to support more features or additional user segments in future phases. "
},
{
	"uri": "//localhost:1313/3-blogstranslated/3.2-blog2/",
	"title": "Streamline access to ISO-rating content changes with Verisk Rating Insights and Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "By Samit Verma, Eusha Rizvi, Manmeet Singh, Troy Smith, Corey Finley, Arun Pradeep Selvaraj, and Ryan Doty – posted on 16 SEP 2025 | Topics: Amazon Bedrock, Amazon ElastiCache, Amazon OpenSearch Service, Artificial Intelligence, Customer Solutions, Generative AI\nThis post is co-written with Samit Verma, Eusha Rizvi, Manmeet Singh, Troy Smith, and Corey Finley from Verisk.\nVerisk Rating Insights, as a feature of ISO Electronic Rating Content (ERC), is a powerful tool designed to provide summaries of ISO Rating changes between two releases.\nTraditionally, extracting specific filing information or identifying differences across multiple releases required manual downloads of full packages, which was time-consuming and prone to inefficiencies.\nThis challenge, coupled with the need for accurate and timely customer support, prompted Verisk to explore innovative ways to enhance user accessibility and automate repetitive processes.\nUsing generative AI and Amazon Web Services (AWS) services, Verisk has made significant strides in creating a conversational user interface for users to easily retrieve specific information, identify content differences, and improve overall operational efficiency.\nIn this post, we dive into how Verisk Rating Insights, powered by Amazon Bedrock, large language models (LLMs), and Retrieval Augmented Generation (RAG), is transforming the way customers interact with and access ISO ERC changes.\nThe challenge\nRating Insights provides valuable content, but there were significant challenges with user accessibility and the time it took to extract actionable insights:\nManual downloading – Customers had to download entire packages to get even a small piece of relevant information. This was inefficient, especially when only a part of the filing needed to be reviewed. Inefficient data retrieval – Users couldn’t quickly identify the differences between two content packages without downloading and manually comparing them, which could take hours and sometimes days of analysis. Time-consuming customer support – Verisk’s ERC Customer Support team spent 15% of their time weekly addressing queries from customers who were impacted by these inefficiencies. Furthermore, onboarding new customers required half a day of repetitive training to ensure they understood how to access and interpret the data. Manual analysis time – Customers often spent 3–4 hours per test case analyzing the differences between filings. With multiple test cases to address, this led to significant delays in critical decision-making. Solution overview\nTo solve these challenges, Verisk embarked on a journey to enhance Rating Insights with generative AI technologies. By integrating Anthropic’s Claude, available in Amazon Bedrock, and Amazon OpenSearch Service, Verisk created a sophisticated conversational platform where users can effortlessly access and analyze rating content changes.\nThe following diagram illustrates the high-level architecture of the solution, with distinct sections showing the data ingestion process and inference loop. The architecture uses multiple AWS services to add generative AI capabilities to the Rating Insights system. The system’s components work together seamlessly, coordinating multiple LLM calls to generate user responses.\nThe following diagram shows the architectural components and the high-level steps involved in the data ingestion process.\nThe steps in the data ingestion process proceed as follows:\nThe process is triggered when a new file is dropped. It is responsible for chunking the document using a custom chunking strategy. This strategy recursively checks each section and keeps them intact without overlap. The process then embeds the chunks and stores them in OpenSearch Service as vector embeddings. The embedding model used in Amazon Bedrock is amazon.titan-embed-g1-text-02. Amazon OpenSearch Serverless is utilized as a vector embedding store with metadata filtering capability. The following diagram shows the architectural components and the high-level steps involved in the inference loop to generate user responses.\nThe steps in the inference loop proceed as follows:\nThis component is responsible for multiple tasks: it supplements user questions with recent chat history, embeds the questions, retrieves relevant chunks from the vector database, and finally calls the generation model to synthesize a response. Amazon ElastiCache is used for storing recent chat history. The embedding model utilized in Amazon Bedrock is amazon.titan-embed-g1-text-02. OpenSearch Serverless is implemented for RAG (Retrieval-Augmented Generation). For generating responses to user queries, the system uses Anthropic’s Claude Sonnet 3.5 (model ID: anthropic.claude-3-5-sonnet-20240620-v1:0), which is available through Amazon Bedrock. Key technologies and frameworks used\nWe used Anthropic’s Claude Sonnet 3.5 (model ID: anthropic.claude-3-5-sonnet-20240620-v1:0) to understand user input and provide detailed, contextually relevant responses. Anthropic’s Claude Sonnet 3.5 enhances the platform’s ability to interpret user queries and deliver accurate insights from complex content changes. LlamaIndex, an open source framework, served as the chain framework for efficiently connecting and managing different data sources to enable dynamic retrieval of content and insights.\nWe implemented RAG, which allows the model to pull specific, relevant data from the OpenSearch Serverless vector database. This means the system generates precise, up-to-date responses based on a user’s query without needing to sift through massive content downloads. The vector database enables intelligent search and retrieval, organizing content changes in a way that makes them quickly and easily accessible. This eliminates the need for manual searching or downloading of entire content packages. Verisk applied guardrails in Amazon Bedrock Guardrails along with custom guardrails around the generative model so the output adheres to specific compliance and quality standards, safeguarding the integrity of responses.\nVerisk’s generative AI solution is a comprehensive, secure, and flexible service for building generative AI applications and agents. Amazon Bedrock connects you to leading FMs, services to deploy and operate agents, and tools for fine-tuning, safeguarding, and optimizing models along with knowledge bases to connect applications to your latest data so that you have everything you need to quickly move from experimentation to real-world deployment.\nGiven the novelty of generative AI, Verisk has established a governance council to oversee its solutions, ensuring they meet security, compliance, and data usage standards. Verisk implemented strict controls within the RAG pipeline to ensure data is only accessible to authorized users. This helps maintain the integrity and privacy of sensitive information. Legal reviews ensure IP protection and contract compliance.\nHow it works\nThe integration of these advanced technologies enables a seamless, user-friendly experience. Here’s how Verisk Rating Insights now works for customers:\nConversational user interface – Users can interact with the platform by using a conversational interface. Instead of manually reviewing content packages, users enter a natural language query (for example, “What are the changes in coverage scope between the two recent filings?”). The system uses Anthropic’s Claude Sonnet 3.5 to understand the intent and provides an instant summary of the relevant changes. Dynamic content retrieval – Thanks to RAG and OpenSearch Service, the platform doesn’t require downloading entire files. Instead, it dynamically retrieves and presents the specific changes a user is seeking, enabling quicker analysis and decision-making. Automated difference analysis – The system can automatically compare two content packages, highlighting the differences without requiring manual intervention. Users can query for precise comparisons (for example, “Show me the differences in rating criteria between Release 1 and Release 2”). Customized insights – The guardrails in place mean that responses are accurate, compliant, and actionable. Additionally, if needed, the system can help users understand the impact of changes and assist them in navigating the complexities of filings, providing clear, concise insights. The following diagram shows the architectural components and the high-level steps involved in the evaluation loop to generate relevant and grounded responses.\nThe steps in the evaluation loop proceed as follows:\nThis component is responsible for calling Anthropic’s Claude Sonnet 3.5 model and subsequently invoking the custom-built evaluation APIs to ensure response accuracy. The generation model employed is Anthropic’s Claude Sonnet 3.5, which handles the creation of responses. The Evaluation API ensures that responses remain relevant to user queries and stay grounded within the provided context. The following diagram shows the process of capturing the chat history as contextual memory and storage for analysis.\nQuality benchmarks\nThe Verisk Rating Insights team has implemented a comprehensive evaluation framework and feedback loop mechanism, as shown in the previous figures, to support continuous improvement and address issues that might arise.\nEnsuring high accuracy and consistency in responses is essential for Verisk’s generative AI solutions. However, LLMs can sometimes produce hallucinations or provide irrelevant details, affecting reliability. To address this, Verisk implemented:\nEvaluation framework – Integrated into the query pipeline, it validates responses for precision and relevance before delivery. Extensive testing – Product subject matter experts (SMEs) and quality experts rigorously tested the solution to ensure accuracy and reliability. Verisk collaborated with in-house insurance domain experts to develop SME evaluation metrics for accuracy and consistency. Multiple rounds of SME evaluations were conducted, where experts graded these metrics on a 1–10 scale. Latency was also tracked to assess speed. Feedback from each round was incorporated into subsequent tests to drive improvements. Continual model improvement – Using customer feedback serves as a crucial component in driving the continuous evolution and refinement of the generative models, improving both accuracy and relevance. By seamlessly integrating user interactions and feedback with chat history, a robust data pipeline is created that streams user interactions to an Amazon Simple Storage Service (Amazon S3) bucket, which acts as a data hub. The interactions then go into Snowflake, which is a cloud-based data platform and data warehouse as a service that offers capabilities such as data warehousing, data lakes, data sharing, and data exchange. Through this integration, the team built comprehensive analytics dashboards that provide valuable insights into user experience patterns and pain points. Although the initial results were promising, they didn’t meet the desired accuracy and consistency levels. The development process involved several iterative improvements, such as redesigning the system and making multiple calls to the LLM. The primary metric for success was a manual grading system where business experts compared the results and provided continuous feedback to improve overall benchmarks.\nBusiness impact and opportunity\nBy integrating generative AI into Verisk Rating Insights, the business has seen a remarkable transformation. Customers have enjoyed significant time savings. By eliminating the need to download entire packages and manually search for differences, the time spent on analysis has been drastically reduced. Customers no longer spend 3–4 hours per test case. What once took days now takes minutes.\nThis time savings brings increased productivity. With an automated solution that instantly provides relevant insights, customers can focus more on decision-making rather than spending time on manual data retrieval. By automating difference analysis and providing a centralized, effortless platform, customers can be more confident in the accuracy of their results and avoid missing critical changes.\nFor Verisk, the benefit is a reduced customer support burden because the ERC customer support team now spends less time addressing queries. With the AI-powered conversational interface, users can self-serve and get answers in real time, freeing up support resources for more complex inquiries.\nThe automation of repetitive training tasks means quicker and more efficient customer onboarding. This reduces the need for lengthy training sessions, and new customers become proficient faster. The integration of generative AI has reduced redundant workflows and the need for manual intervention. This streamlines operations across multiple departments, leading to a more agile and responsive business.\nConclusion\nLooking ahead, Verisk plans to continue enhancing the Rating Insights platform in two ways. First, they will expand the scope of queries, enabling more sophisticated queries related to different filing types and more nuanced coverage areas. Second, they will scale the platform. With Amazon Bedrock providing the infrastructure, Verisk aims to scale this solution further to support more users and additional content sets across various product lines.\nVerisk Rating Insights, now powered by generative AI and AWS technologies, has transformed the way customers interact with and access rating content changes. Through a conversational user interface, RAG, and vector databases, Verisk intends to eliminate inefficiencies and save customers valuable time and resources while enhancing overall accessibility. For Verisk, this solution has improved operational efficiency and provided a strong foundation for continued innovation.\nWith Amazon Bedrock and a focus on automation, Verisk is driving the future of intelligent customer support and content management, empowering both customers and internal teams to make smarter, faster decisions.\nFor more information, refer to the following resources:\nExplore generative AI on AWS Learn about unlocking the business value of generative AI Learn more about Anthropic’s Claude 3 models on Amazon Bedrock Learn about Amazon Bedrock and how to build and scale generative AI applications with FMs Explore generative AI quick start proofs of concept About the authors\nSamit Verma serves as the Director of Software Engineering at Verisk, overseeing the Rating and Coverage development teams. In this role, he plays a key part in architectural design and provides strategic direction to multiple development teams, enhancing efficiency and ensuring long-term solution maintainability. He holds a master’s degree in information technology. Eusha Rizvi serves as a Software Development Manager at Verisk, leading several technology teams within the Ratings Products division. Possessing strong expertise in system design, architecture, and engineering, Eusha offers essential guidance that advances the development of innovative solutions. He holds a bachelor’s degree in information systems from Stony Brook University. Manmeet Singh is a Software Engineering Lead at Verisk and AWS Certified Generative AI Specialist. He leads the development of an agentic RAG-based generative AI system on Amazon Bedrock, with expertise in LLM orchestration, prompt engineering, vector databases, microservices, and high-availability architecture. Manmeet is passionate about applying advanced AI and cloud technologies to deliver resilient, scalable, and business-critical systems. Troy Smith is a Vice President of Rating Solutions at Verisk. Troy is a seasoned insurance technology leader with more than 25 years of experience in rating, pricing, and product strategy. At Verisk, he leads the team behind ISO Electronic Rating Content, a widely used resource across the insurance industry. Troy has held leadership roles at Earnix and Capgemini and was the cofounder and original creator of the Oracle Insbridge Rating Engine. Corey Finley is a Product Manager at Verisk. Corey has over 22 years of experience across personal and commercial lines of insurance. He has worked in both implementation and product support roles and has led efforts for major carriers including Allianz, CNA, Citizens, and others. At Verisk, he serves as Product Manager for VRI, RaaS, and ERC. Arun Pradeep Selvaraj is a Senior Solutions Architect at Amazon Web Services (AWS). Arun is passionate about working with his customers and stakeholders on digital transformations and innovation in the cloud while continuing to learn, build, and reinvent. He is creative, energetic, deeply customer-obsessed, and uses the working backward process to build modern architectures to help customers solve their unique challenges. Connect with him on LinkedIn. Ryan Doty is a Solutions Architect Manager at Amazon Web Services (AWS), based out of New York. He helps financial services customers accelerate their adoption of the AWS Cloud by providing architectural guidelines to design innovative and scalable solutions. Coming from a software development and sales engineering background, the possibilities that the cloud can bring to the world excite him. "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "//localhost:1313/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Finalize the Proposal. Continue developing Lambda functions and complete the planned features. Complete the data standardization workflow, RAG techniques, chunking process, and automation pipelines. Attend AWS events to update knowledge on AI/ML/GenAI. Tasks completed during the week: Day Task Start Date Completion Date Reference Material 2 Finalized the Proposal and architecture based on feedback. 10/11/2025 11/11/2025 3 Weekly team meeting and progress reporting. 11/11/2025 11/11/2025 4 - 6 Data collection and standardization, testing embedding and RAG code Developing Lambda functions 12/11/2025 14/11/2025 7 Event – AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS Figma: Review \u0026amp; Analysis pages 15/11/2025 15/11/2025 Sun Figma: Review \u0026amp; Analysis pages (continued) 16/11/2025 16/11/2025 Week 10 Achievements: Finalized Proposal \u0026amp; Architecture: Completed the final version of the Proposal document and architectural diagram after incorporating feedback. Advanced Knowledge Update: Attended the AWS Cloud Mastery Series event and gained valuable insights into AI/ML and GenAI technologies on AWS. "
},
{
	"uri": "//localhost:1313/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Attend AWS events to enhance DevOps knowledge. Continue developing Lambda functions, testing, and debugging. Regularly gather feedback to refine and complete AI features. Tasks completed during the week: Day Task Start Date Completion Date Reference Material 2 Event – AWS Cloud Mastery Series #2: DevOps on AWS 17/11/2025 17/11/2025 3–6 Completed the Lambda functions for RAG retrieval, LLM invocation, and other supporting features; performed debugging and created test cases. 18/11/2025 21/11/2025 7 Updated UI/UX page designs based on feedback. 22/11/2025 23/11/2025 Week 11 Achievements: Enhanced DevOps Knowledge: Attended the AWS event and gained insights into best practices for DevOps. Completed Lambda Functions Ahead of Schedule: Developed nearly all required Lambda functions… "
},
{
	"uri": "//localhost:1313/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Finalize and refine key functional pages such as Analysis, Contract Editor, and Upload. Attend an AWS event to learn about the Security Pillar of the AWS Well-Architected Framework. Fix bugs and improve system response time. Tasks completed during the week: Day Task Start Date Completion Date Reference Material 2 Continued fixing remaining issues Finalized and tested core features Coordinated deployment of the entire system to AWS 24/11/2025 28/11/2025 7 Event – AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar 29/11/2025 29/11/2025 Sun Fixed remaining issues 30/11/2025 30/11/2025 Week 12 Achievements: Strong Understanding of AWS Security Principles: Participated in the event and gained valuable knowledge about the AWS Well-Architected Security Pillar, supporting the development of a secure and reliable system. Improved Navigation \u0026amp; Issue Resolution: Significantly enhanced application navigation and resolved UI-related issues, ensuring smoother system operation. "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.3-event3/",
	"title": "AWS Cloud Mastery Series #1",
	"tags": [],
	"description": "",
	"content": "AI/ML/GenAI on AWS Saturday, 15/11/2025 – AWS Vietnam Office I. General information about the event Event name: AI/ML/GenAI on AWS\nTime: 8:30 – 12:00, 15/11/2025\nVenue: AWS Vietnam Office\nObjectives:\nProvide an overview of AI/ML and GenAI on the AWS platform. Clarify the concept of foundation models and practical application patterns. Present the AI/ML service ecosystem on AWS, from “ready-made” services to platforms for building custom models. Demonstrate how to build GenAI applications using Amazon Bedrock, RAG, and AgentCore. II. Main contents by timeline 1. Opening session – Introduction to GenAI and Foundation Models (8:30 – 9:00) Presenter Lâm Tuấn Kiệt focused on laying the foundation for Generative AI (GenAI):\nWhat is GenAI:\nUnlike traditional ML models that mainly “classify” or “predict”, GenAI can generate new content: text, images, audio, source code, etc. GenAI is built on very large-scale foundation models (billions of parameters), trained on diverse datasets, then fine-tuned or instruction-tuned for specific real-world tasks. Foundation Models \u0026amp; Amazon Bedrock:\nAmazon Bedrock was introduced as a platform to manage and access multiple foundation models from different providers (such as Claude, Llama, Titan, …), allowing users to avoid managing infrastructure, scaling, or model security themselves. Key point: through a single API, you can experiment with multiple models, compare quality and cost, and select the right model for each scenario (chatbot, summarization, text classification, RAG, …). Prompt Engineering:\nEmphasized as a core skill when working with GenAI:\nPrompt engineering = the process of designing, experimenting with, and refining prompts so that the model correctly understands the context and produces appropriate outputs. Main techniques mentioned:\nZero-shot prompting: no examples provided, only a description of the requirement. Few-shot prompting: provide a few sample examples so the model “learns how to respond” in the desired style. Chain-of-Thought (CoT): instruct the model to lay out its reasoning step by step, which improves logical consistency, especially for tasks involving calculations and multi-step reasoning. RAG (Retrieval-Augmented Generation):\nIntroduced as an architecture that combines knowledge retrieval with GenAI: the model is mainly “good at language”, while specific knowledge is taken from the organization’s own data sources (documents, knowledge bases, …).\nMy notes emphasize: “RAG: retrieving relevant info from a data source” – the model will retrieve relevant text passages, then use a foundation model to synthesize and answer based on that data, helping to:\nReduce the risk of “hallucination”. Make knowledge updates easier (just update the data store, without retraining the model). 2. “AWS AI/ML Services Overview” \u0026amp; Embeddings/RAG in Action (9:00 – 10:30) This part focused on the overall picture of the AI service ecosystem on AWS and how to leverage it for GenAI/RAG.\n2.1. Embeddings and Titan Embeddings In the slide “What are embeddings?”, the instructor explained:\nEmbeddings are a way to represent text/images as numerical vectors so that models can measure similarity, perform search, clustering, etc. Embeddings are the “foundation” of RAG systems, semantic search, recommendation systems, … AWS introduced Titan Embeddings – an embedding option provided by AWS, optimized for:\nSemantic search. RAG over enterprise documents. Tight integration with Amazon Bedrock and other services. Note: “Embeddings. Some options supported by AWS. RAG in action.” – I understood this as a demo describing the pipeline: data → chunking → generate embeddings → store in a vector store → query and combine with an LLM to answer.\n2.2. “Ready-made” AI services on AWS Mr. Hoàng Anh provided details on managed AI services, where I captured both use cases and indicative pricing:\nAmazon Rekognition – Computer Vision:\nDetects objects, faces, and text in images/videos. Note: “0.0013 USD/image (under 1 million images)” → suitable for medium-scale image recognition use cases such as camera monitoring and content filtering. Amazon Translate – Neural Machine Translation:\nAutomatic multilingual translation. Note: “15 USD / 1 million characters” → can be applied to automated translation of documents, emails, and web content. Amazon Textract – OCR \u0026amp; structured document extraction:\nNot only recognizes text but also preserves layout of tables, forms, and fields. Note: “0.05 USD/page (under 1 million pages)”, highly suitable for extracting data from contracts and invoices. Amazon Transcribe – Speech-to-Text:\nConverts speech to text, supports captions and meeting transcripts. Note: “0.024 USD/minute (under 250k minutes)”. Amazon Polly – Text-to-Speech:\nGenerates natural-sounding speech from text. Note: “4 USD / 1 million characters” → used for voicebots, audio guides, and training videos. Amazon Comprehend – NLP service:\nAnalyzes sentiment, key phrases, entities, relationships, … from text. Note: “0.0001 USD / 100 characters or 3 USD/hour” → suitable for customer feedback analysis systems and ticket classification. Amazon Kendra – Intelligent Search / RAG Support:\nSemantic search, FAQ, semantic retrieval. Note: “30 USD/index/month + 0.35 USD/hour” → acts as an internal “search engine” for enterprise documents, integrates well with RAG. Amazon Personalize – Recommendation:\nPersonalized recommendations: products, content, user segmentation, … Note: “0.24 USD/hour training + 0.05 USD/GB + 0.15 USD/1000 recommendations”. Listing use cases along with cost made it easier for me to envision designing solutions that are both technically appropriate and budget-conscious.\n3. “Generative AI with Amazon Bedrock” – AgentCore \u0026amp; Pipecat (10:45 – 12:00) After the overview, the program moved into building practical GenAI systems.\n3.1. Pipecat – Framework for voice/multimodal AI agents Pipecat was introduced as a pipeline framework optimized for real-time voice/multimodal agents:\nSupports composing multiple components: speech-to-text, LLM, text-to-speech, external tools, … into a unified pipeline. Optimized for low latency, suitable for voice assistants and callbot applications. The key takeaway for me: instead of “manually wiring” every service, one can use a framework like Pipecat to standardize the processing flow, making it easier to scale and maintain.\n3.2. Amazon Bedrock AgentCore and the Agentic AI ecosystem Speaker Hiếu Nghị presented Bedrock AgentCore and the broader picture of agentic AI systems:\nFrom LLMs to Agentic Systems:\nA standalone LLM only “answers questions”. An agentic system adds: goals, planning, tool calling capabilities, data access, and multi-step decision-making. Bedrock AgentCore was introduced as an orchestration layer for building such agents on AWS infrastructure. Agent-building frameworks mentioned:\ncrew.ai, Google ADK, LlamaIndex, OpenAI Agents SDK, LangChain, LangGraph, Strands Agents SDK, … Main point: the ecosystem is rich, but when running on AWS, AgentCore helps optimize integration with AWS services, security, logging, monitoring, … Key capabilities of Bedrock AgentCore (based on slides and notes):\nConnects to multiple foundation models on Bedrock. Manages multi-step workflows and tool calls (Lambda, internal APIs, data queries, …). Supports RAG and integrates with Kendra/Knowledge Base. Enables guardrails (content policies, domain restrictions for responses). Provides observability \u0026amp; monitoring over agent behavior in production. Challenges when putting agents into production:\nStability and predictability of agent behavior. Controlling inference costs when workflows become complex. Data security and access control when agents call into multiple systems. Detailed logging/monitoring for debugging and continuous improvement. The session concluded with a demo of building a GenAI chatbot on Bedrock using RAG and basic guardrails, bridging theory with practical implementation.\nIII. Knowledge and lessons learned Clearer understanding of the role of GenAI in modern system architecture\nGenAI does not stand alone; it must be combined with RAG, search, data pipelines, monitoring, … Proper understanding of foundation models and prompt engineering helps avoid unrealistic expectations and leads to more practical solution design. Grasping the AI service ecosystem on AWS from a “build vs. buy” perspective\nServices like Rekognition, Textract, Comprehend, Personalize, … are well suited for standardized use cases that do not require deep customization. When domain-specific data is involved (contracts, legal documents, internal logs, …), combining Bedrock + Embeddings + Kendra/RAG offers more flexibility. Pragmatic view on costs\nThe concrete numbers in the notes make it clear that technical problems must always be considered alongside economic constraints. Right from the design phase, it is necessary to estimate volume (number of images, characters, minutes of audio, requests) to choose appropriate services and models. “Agentic” mindset instead of just “chatbot”\nThe concept of AgentCore and agentic AI systems shows that the next step is not just Q\u0026amp;A, but process automation: reading data, calling APIs, making decisions, writing logs, …\nThis opens many directions for personal projects:\nAn agent assisting with legal contract analysis. An agent monitoring infrastructure logs and alerting on anomalies. An agent assisting with financial analysis and suggesting actions based on real-time data. IV. Personal application roadmap After this event, I plan to:\nDesign a RAG pipeline on AWS for my current problem domain (e.g., legal contract analysis, educational materials):\nUse Textract to extract document content. Use Titan Embeddings to generate vectors and store them in a vector store. Connect Kendra or an equivalent search engine for retrieval. Use Bedrock (Claude/Llama/Titan) to build a chatbot that answers based on internal documents. Experiment with a small Agent using Bedrock AgentCore:\nInitial step: the agent receives a question → queries the knowledge base → synthesizes → produces a short report. Next step: extend so that the agent can call Lambda to execute actions (e.g., create tickets, send emails, update logs). Further develop prompt engineering skills:\nPractice different prompt types: zero-shot, few-shot, CoT on my own use cases (AI/ML, DevOps, Security). Record effective “prompt patterns” into a reusable library for multiple projects. V. Images "
},
{
	"uri": "//localhost:1313/3-blogstranslated/3.3-blog3/",
	"title": "How Poland’s Post Bank accelerated digital transformation while maintaining regulatory compliance on AWS",
	"tags": [],
	"description": "",
	"content": "Authors: Waldemar Szczepański, Bartłomiej Rafał, Piotr Boetzel, and Dariusz Matczak | Posted: 16/09/2025 | Categories: Cloud Adoption, Customer Solutions, Financial Services, Government, Migration, Public Sector, Regions\nIn Poland’s competitive financial services market, Post Bank faced a critical challenge: how to accelerate innovation and improve customer experience while operating under strict regulatory requirements. The answer came through a strategic cloud migration that transformed not only the bank’s technology infrastructure, but also its entire approach to digital banking.\nBy migrating its electronic banking system to Amazon Web Services (AWS), Post Bank reduced application deployment time from 2 hours to just 10 minutes, lowered CPU utilization by 40%, and significantly improved system reliability—all while maintaining full compliance with Poland’s stringent financial regulations.\nThis transformation story demonstrates how financial institutions can use AWS Cloud technology to become more agile and efficient without compromising security or compliance. For Post Bank, the impact goes beyond technical metrics: employee turnover fell from 30% to 5%, and the bank can now provision new development environments in 30 minutes instead of 30 days.\nBuilding confidence through incremental adoption Post Bank’s cloud journey began cautiously in 2019 with the migration of a single noncritical system.\n“We needed to learn cloud technologies and build confidence across the organization,” — Waldemar Szczepański, Head of the Cloud Center of Excellence (CCoE) at Post Bank.\nThis controlled approach allowed the IT team to develop cloud skills while demonstrating tangible value to stakeholders.\nThe COVID-19 pandemic significantly accelerated the bank’s digital transformation. The ability to scale quickly and roll out new features continuously became critical to business success.\nIn addition, the conflict in Ukraine introduced another strategic consideration: senior leadership recognized the importance of geographic redundancy and hosting systems outside Poland’s borders.\nThese converging factors created the right moment for organizational change. Post Bank established a CCoE team following the AWS Cloud Adoption Framework (AWS CAF), laying the foundation for a comprehensive cloud migration strategy.\nNavigating organizational transformation Migrating a critical business system requires more than technical expertise—it demands organizational alignment. Post Bank’s transformation reshaped the entire IT operating model: adopting new technology stacks, changing ways of working, introducing cloud cost management practices, and updating operational processes.\nThe CCoE team had to coordinate a wide range of internal stakeholders, including architecture, security, audit, and operations teams — each with its own requirements and concerns that needed to be addressed.\n“We couldn’t do this alone,” — Szczepański explains. “AWS architects and an AWS Partner helped us build the proof of concept, and the AWS Migration Acceleration Program (MAP) provided both methodology and partial funding for the migration.”\nThis collaborative approach proved essential. AWS architects and experts from the AWS Partner worked side by side with Post Bank’s teams, contributing deep expertise while transferring knowledge to internal staff.\nArchitecting for a hybrid environment After carefully assessing team capabilities, application complexity, and user impact, the CCoE team selected the electronic and mobile banking application as the flagship migration project. This mission-critical system would test the bank’s ability to maintain performance and reliability in a hybrid environment spanning on-premises and cloud.\nThe team adopted Infrastructure as Code (IaC) with Terraform as the foundation for fast and consistent deployments.\nHowever, the hybrid architecture introduced its own challenges. The 750-kilometer distance between the on-premises data center and the AWS Region Europe (Frankfurt) made latency management a top priority.\nPost Bank deployed redundant AWS Direct Connect links over geographically diverse paths, achieving stable 30-millisecond latency that met the application’s operational requirements.\nFigure 1: AWS Direct Connect connectivity layout\nThe project team applied the 6 Rs migration strategies pragmatically: rehost, replatform, repurchase, re-architect, retire, and retain.\nWhen the on-premises network access control model couldn’t be directly replicated in the cloud, they chose the “repurchase” strategy and adopted a third-party solution from AWS Marketplace.\nTo ensure high availability for the database layer, they replatformed to Amazon Relational Database Service (Amazon RDS) Multi-AZ deployments. The in-memory cache solution was rehosted and enhanced with a community plugin to support AWS Auto Scaling.\nProving value through measurable success Post Bank’s proof of concept (PoC) was more than a technical test—it was a data-driven mechanism to convince stakeholders. The project team defined 10 key performance indicators (KPIs) that directly addressed stakeholder concerns around cost, security, and performance.\n“Choosing the right KPIs was critical,” — notes Bartłomiej Rafał, CCoE Technical Lead. “We needed metrics that could respond to skepticism with hard numbers.”\nThe results exceeded expectations for most metrics. Only one KPI fell short of the target, yet it still represented a significant improvement over the average on-premises performance.\nThis evidence-based approach turned skeptics into strong supporters. System availability improved noticeably thanks to automatic self-healing capabilities, which resolve incidents within 10 seconds instead of requiring manual intervention. Development velocity increased sharply, with new environments now provisioned in 30 minutes instead of 30 days.\nMaintaining security and compliance in the cloud For a financial institution in Poland, security and regulatory compliance are non-negotiable. Post Bank built its cloud foundation on AWS best practices, following the AWS Well-Architected Framework Security Pillar and the AWS Security Reference Architecture.\nBy using AWS Organizations together with AWS IAM Identity Center and service control policies (SCPs), the bank enforced critical compliance controls such as environment isolation, separation of duties, least privilege access, and mandatory encryption. AWS Control Tower simplified security governance, enabling policies such as restricting service usage to Regions within the European Economic Area (EEA).\nThe engineering team used Account Factory for Terraform (AFT) to create all new AWS accounts with standardized configuration and security settings. For identity management, they federated IAM Identity Center with the existing identity management system, simplifying compliance audits by adjusting existing processes rather than introducing entirely new ones. The following diagram illustrates this architecture:\nFigure 2: Permissions assignment and security architecture\nIt was important for the bank to keep changes to existing processes to a minimum while still simplifying compliance. The hub-and-spoke network security architecture, shown in the following diagram, allowed the bank to extend its existing security processes into the AWS Cloud through synchronized firewall management.\nFigure 3: Network architecture and security management\nLessons for financial institutions Post Bank’s successful cloud migration offers valuable lessons for other financial institutions planning similar journeys:\nStart small but think big – Beginning with a noncritical system helped Post Bank build skills and confidence while limiting risk.\nEstablish strong governance early – The CCoE team played a central role in leading the initiative and coordinating across stakeholder groups.\nInvest in architecture – Time spent designing the system correctly, including evaluating 6 Rs migration strategies, pays off during implementation and operations.\nUse PoCs strategically – Include KPIs that directly reflect stakeholder concerns and clearly demonstrate benefits such as higher availability and better operational efficiency.\nLeverage external expertise – Collaborate with AWS architects and use programs like MAP (Migration Acceleration Program) to accelerate migration while building internal capabilities.\nLooking ahead: Continuous innovation “AWS Cloud made our administrators and testers happier and increased the satisfaction of our business stakeholders because we can deploy changes and upgrades faster,”\nsays Szczepański. This transformation has fundamentally changed how Post Bank approaches technology. Artur Szatkowski, Director of the IT Systems Department at Post Bank, states confidently:\n“We will not go back to on-premises solutions.”\nThe bank plans to migrate additional systems and is actively exploring new cloud-based capabilities. Recently, it implemented an internal AI assistant using Amazon Bedrock and Anthropic’s Claude 3.5. This allows employees to quickly search across the bank’s extensive internal knowledge base, including documents, forms, terms of service, and marketing materials.\nPost Bank’s journey shows that with careful planning, close collaboration, and a strong commitment to best practices, financial institutions can gain the agility and innovation of cloud computing while maintaining the strict security and compliance demanded by customers and regulators.\nAbout Post Bank Post Bank is a Polish consumer bank with approximately 700,000 customers and 35 years of market presence. Its strategic partner and majority shareholder is the national Polish Post. Through this partnership, the bank’s products and services are available at every post office nationwide, creating a network of around 4,700 branches—five times larger than its competitors. This reach enables Post Bank to serve even citizens who are not digitally connected, supporting financial inclusion across Poland.\nWaldemar Szczepański – Waldemar is the Head of the Cloud Center of Excellence (CCoE) at Post Bank, responsible for the bank’s development in cloud technologies and artificial intelligence (AI). He has over 20 years of experience in the financial sector. At Post Bank, he has led projects to build a modern workplace and to adopt new technologies, including AI and cloud banking. Bartłomiej Rafał – Bartłomiej is the CCoE Technical Lead at Post Bank. He is passionate about using technology to solve business problems and improve existing processes. As a tech generalist, he has a broad interest in all areas of IT—from infrastructure, cybersecurity, and system architecture to management—and always has more ideas than time to implement them. Piotr Boetzel – Piotr is a Senior Solutions Architect at AWS, working with public sector customers in Central and Eastern Europe (CEE). He supports customers in modernization and transformation projects, with a particular focus on security and regulatory compliance. Dariusz Matczak – Dariusz is an Account Manager at AWS, responsible for public sector customers in Poland. He has over 15 years of experience working with clients and partners across multiple industries, supporting their digital transformation to the cloud and helping deliver a variety of technology projects. "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "//localhost:1313/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Getting started with healthcare data lakes: Using microservices This blog explains how to build a modern healthcare data lake on AWS using a microservices architecture. It discusses why healthcare organizations need a central data platform to consolidate diverse sources such as electronic health records (EHR), lab systems, imaging, and IoT medical devices. The article shows how microservices—exposed via APIs and deployed on containers or serverless—help decouple ingestion, transformation, analytics, and governance, making the platform easier to scale and evolve. It walks through typical patterns for landing data in Amazon S3, cataloging and transforming it with services like AWS Glue, enforcing security and privacy (for example, encryption, fine-grained access control, logging), and aligning the architecture with regulations such as HIPAA.\nBlog 2 - Streamline access to ISO-rating content changes with Verisk Rating Insights and Amazon Bedrock This blog describes how Verisk enhanced its ISO Electronic Rating Content (ERC) with a generative AI–powered conversational interface using Amazon Bedrock. It starts from customer pain points—manual downloads of full rating packages, slow comparison of releases, and heavy load on the ERC support team—and shows how Verisk built a Retrieval-Augmented Generation (RAG) system. Documents are chunked and embedded with the Amazon Titan embedding model, stored in Amazon OpenSearch Serverless as vector embeddings, and retrieved dynamically based on user queries. Anthropic Claude 3.5 Sonnet on Amazon Bedrock generates responses, while Amazon ElastiCache stores chat history and LlamaIndex orchestrates data sources. The post also covers evaluation loops, guardrails with Amazon Bedrock Guardrails, SME-driven quality metrics, and an analytics pipeline using Amazon S3 and Snowflake. Business results include cutting analysis time from hours or days to minutes, reducing support workload, speeding onboarding, and giving customers more accurate, explainable insights into rating content changes.\nBlog 3 - How Poland’s Post Bank accelerated digital transformation while maintaining regulatory compliance on AWS This blog tells the story of how Poland’s Post Bank modernized its digital banking platforms on AWS while staying compliant with strict financial regulations. Starting with a cautious migration of a noncritical system in 2019, the bank gradually built cloud skills and confidence, then selected its electronic and mobile banking application as the flagship migration project. Using Terraform-based Infrastructure as Code, a hybrid architecture with redundant AWS Direct Connect links to the Europe (Frankfurt) Region, and a mix of 6R migration strategies (rehost, replatform, repurchase, etc.), the bank achieved predictable latency, high availability on Amazon RDS Multi-AZ, and scalable caching with Auto Scaling. The article highlights strong governance based on AWS Organizations, IAM Identity Center, SCPs, AWS Control Tower, and a hub-and-spoke network security model aligned to the AWS Well-Architected Framework and Security Reference Architecture. Measurable outcomes include reducing deployment time from 2 hours to 10 minutes, cutting CPU usage by 40%, shrinking environment provisioning from 30 days to 30 minutes, and lowering employee turnover from 30% to 5%. The bank now plans further migrations and has already deployed an internal AI assistant using Amazon Bedrock and Anthropic Claude 3.5 to search internal knowledge, demonstrating continuous innovation on a secure, compliant cloud foundation.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.4-event4/",
	"title": "AWS Cloud Mastery Series #2",
	"tags": [],
	"description": "",
	"content": "DevOps on AWS Monday, 17/11/2025, from 8:30 – 17:00 – AWS Vietnam Office I. General information about the event Event name: AWS Cloud Mastery Series #2 – DevOps on AWS\nTime: Monday, 17/11/2025, from 8:30 to 17:00\nVenue: AWS Vietnam Office\nParticipants: Students, interns, and junior engineers interested in DevOps, AWS infrastructure, and operating systems with an automation-first approach.\nMain objectives of the session:\nGain a proper understanding of the DevOps mindset and the role of DevOps/Platform Engineers within an organization. Understand the DevOps Services ecosystem on AWS (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and how to build a complete CI/CD pipeline. Get familiar with the Infrastructure as Code (IaC) mindset through CloudFormation and AWS CDK, and understand why “ClickOps” is no longer appropriate at scale. Understand the different container service options on AWS (ECR, ECS, EKS, App Runner) and deployment strategies. Grasp the concepts and tools of Monitoring \u0026amp; Observability (CloudWatch, X-Ray, Grafana, Prometheus) to operate systems proactively. II. Detailed agenda for the day 2.1. Morning session – DevOps Mindset \u0026amp; CI/CD \u0026amp; IaC 2.1.1. 8:30 – 9:00 | Welcome \u0026amp; DevOps Mindset (Quang Tịnh – Platform Engineer)\nAt the beginning, speaker Quang Tịnh briefly revisited the previous AI/ML session and raised the core question: DevOps emerged to address the gap between Development (Dev) and Operations (Ops) – “DevOps is the bridge between Dev and Ops”, not just a job title but a mindset and a way of working.\nThe speaker emphasized several characteristics of DevOps culture:\nCollaboration: Dev and Ops work as one team, jointly responsible for product quality and operations. Automation-first: prioritize automated build, test, and deployment instead of manual operations. Measurement \u0026amp; Feedback: always measure and respond based on data, not intuition. Regarding DevOps metrics, the speaker revisited the common DORA-oriented metrics:\nDeployment Frequency – how often deployments occur. Lead Time for Changes – time from commit to production. MTTR (Mean Time To Recovery) – average time to recover after an incident. Change Failure Rate – percentage of deployments that result in failures. At the end of this part, the speaker linked to the role of the Platform Engineer: building the platform, tools, and standard processes that enable product teams to deploy, monitor, and operate services consistently with minimal friction.\n2.1.2. 9:00 – 10:30 | AWS DevOps Services – CI/CD Pipeline (Kha – CI/CD Workflow)\nThis section focused on the CI/CD pipeline on AWS, presented by Kha.\nThe basic pipeline was described with the following main components:\nSource Control – AWS CodeCommit \u0026amp; Git strategies\nCodeCommit was introduced as a managed Git service on AWS, equivalent to GitHub/GitLab but with deep integration into the AWS ecosystem.\nPopular Git strategies were mentioned:\nGitFlow: suitable for workflows with large releases and multiple feature branches. Trunk-based Development: frequent commits to main/trunk, optimized for CI/CD and fast deployments. Build \u0026amp; Test – AWS CodeBuild\nUse CodeBuild to build source code and run unit tests and integration tests. The buildspec file defines steps to install dependencies, build, test, and output artifacts. Deployment – AWS CodeDeploy\nDeployment strategies introduced:\nBlue/Green: run two environments in parallel and switch traffic once the new version is stable. Canary: shift traffic in small increments to reduce risk. Rolling update: gradually update groups of instances. Orchestration – AWS CodePipeline\nCodePipeline acts as the “backbone” of the pipeline, connecting CodeCommit → CodeBuild → CodeDeploy and additional steps for approvals and automated tests. The overall pipeline model was presented on slides and illustrated via a demo walk-through (from commit to successful deployment). The key highlight was combining the DevOps mindset with the AWS toolchain: every step from commit to production should be automated and monitored.\n2.1.3. 10:45 – 12:00 | Infrastructure as Code (IaC) – Thịnh Nguyễn \u0026amp; Hoàng Anh\nAt the beginning of the session, speakers Thịnh Nguyễn and Hoàng Anh posed the question:\n“Why ClickOps isn’t ideal?” – Why should we not manage infrastructure by manually clicking on the console?\nFrom there, the reasons were listed:\nAutomation: you cannot automate when every operation is a manual click. Scalability: as systems grow and resources multiply, humans cannot manage everything manually. Reproducibility: difficult to recreate identical environments across dev, staging, and production. Collaboration: there is no single “source of truth”, making review, audit, and error prevention difficult. AWS CloudFormation\nIntroduced as the standard IaC service on AWS, using templates (YAML/JSON) to define resources.\nKey concepts:\nTemplate: a file defining the resources to be created (VPC, EC2, S3, RDS, etc.). Stack: one deployment of a template. Drift detection: detects differences between actual infrastructure and the template (when someone does “ClickOps” in the console). Advantages: can be version-controlled; supports stack rollback on failure; makes environment replication easier.\nAWS Cloud Development Kit (CDK – “CloudDevKit”)\nMentioned as a modern IaC option, using programming languages (TypeScript, Python, Java, etc.) to define infrastructure.\nCDK generates CloudFormation templates and enables:\nReusing constructs (packaged infrastructure components). Applying AWS best practices and standard patterns. The “Choosing between IaC tools” section concluded:\nCloudFormation: suitable when you want to stay close to “native AWS”, simple and easy to control. CDK: suitable for larger projects requiring higher abstraction, heavy reuse, and development teams already comfortable with coding. 2.2. Afternoon session – Container Services \u0026amp; Monitoring/Observability 2.2.1. 13:00 – 14:30 | Container Services on AWS (Trần Vĩ)\nSpeaker Trần Vĩ started with basic concepts:\nContainer: an application “packaged” with all its dependencies so it can run consistently across different environments. Distinguishing Container vs Virtual Machine (VM): containers share the kernel, are lighter, and start faster; VMs provide stronger isolation but are heavier and consume more resources (detailed comparison was on the slides; not fully captured in notes). Docker \u0026amp; Docker Workflow\nContainer engine: Docker. Dockerfile: defines how to build the image and the runtime environment. Image: a “blueprint” packaging code and runtime. Basic workflow: write Dockerfile → build image → push to registry → run container. Amazon ECR (Elastic Container Registry)\nIntroduced as a fully managed container registry, private by default.\nKey features from the notes:\nImage scanning Immutable tags Lifecycle policies Encryption \u0026amp; IAM to control access to images. Amazon ECS (Elastic Container Service)\nA container orchestration service.\nFunctions: ensures containers automatically restart on failure, scales up/down, distributes traffic across services, and manages containers across multiple servers.\nSupports two modes:\nEC2 mode: run containers on an EC2 cluster – cost-effective for long-running workloads but requires managing servers. Fargate mode: serverless, no infrastructure management, pay only for resources used. Main components: ECS cluster, task definition, task, service – architectural diagrams were shown on the slides.\nAmazon EKS (Elastic Kubernetes Service)\nManaged Kubernetes service, used when a flexible and complex architecture is required. Components: Control plane (master), worker node, pod, service, with diagrams illustrating Kubernetes architecture. ECS vs EKS comparison: ECS is simpler and tightly integrated with AWS; EKS is more flexible and suitable when teams already have Kubernetes experience or require higher portability. AWS App Runner\nNotes recorded:\n“Fast, simple, cost effective” “Directly from source code, no management required” App Runner is suitable when you want to get web/services running as quickly as possible without managing clusters, nodes, etc.\n2.2.2. 14:45 – 16:00 | Monitoring \u0026amp; Observability (Nghiêm, Long, Quý)\nConcepts of Monitoring \u0026amp; Observability – by Nghiêm\nMonitoring: tracking system metrics, logs, and alerts.\nObservability: the ability to infer the root cause of incidents based on data collected from the system (metrics, logs, traces).\nRelated AWS services:\nAmazon CloudWatch Amazon Managed Grafana AWS X-Ray Amazon Managed Service for Prometheus Amazon CloudWatch\nMonitors AWS resources in real time, providing metrics and logs for both AWS services and on-premises resources. Metrics: performance data of the system; can be default or custom metrics. Logs: collects logs from multiple AWS services, stores them, and allows queries for analysis. Alarms: configure thresholds to automatically send notifications (SNS, email, etc.) or trigger actions (Auto Scaling, EventBridge). Dashboards: visual interfaces combining metrics and logs, supporting operational and cost optimization. AWS X-Ray – by Long\nFocuses on microservices systems and helps:\nAnalyze performance bottlenecks. Perform distributed tracing: track requests end-to-end, visualize service maps, and propagate trace IDs across services. Support root cause analysis and real user monitoring (RUM) when combined with CloudWatch. Observability Best Practices – by Quý\nDetailed content was on the slides; only the headings were noted. Focused on standardizing how to log, measure metrics, and trace, setting reasonable alerts, avoiding “alert noise”, and integrating with DevOps/SRE workflows. 2.2.3. 16:00 – 17:00 | DevOps Best Practices, Career \u0026amp; Q\u0026amp;A\nThe final session of the day mainly summarized:\nRecap of the standard pipeline: from DevOps mindset → IaC → CI/CD → Containers → Observability. Reminder of DevOps best practices: safe deployments (feature flags, canary, blue/green), automated testing, and blameless postmortems. Discussion on DevOps career pathways and the AWS Certifications roadmap (Developer Associate, SysOps, DevOps Engineer Professional, etc.). III. Knowledge and lessons learned DevOps is a holistic mindset, not just a set of tools\nEmphasizing that DevOps is “the bridge between Dev and Ops” helped me better understand the role of DevOps/Platform Engineers: designing shared platforms for teams, ensuring that code-to-production flows through a standardized, measurable, and automated pipeline. The importance of CI/CD and a clear Git strategy\nUsing CodeCommit + CodeBuild + CodeDeploy + CodePipeline allows standardization of the build–test–deploy process. Choosing between GitFlow and trunk-based is not just about branch naming; it directly affects release frequency, hotfix handling, and rollback strategies. Infrastructure as Code thoroughly addresses ClickOps drawbacks\nThe reasons “Automation – Scalability – Reproducibility – Collaboration” make me reassess the habit of clicking through the console. CloudFormation is suitable when infrastructure needs to be tightly controlled in an AWS-native manner, while CDK is appropriate when seeking reuse and code-based organization of infrastructure. A clearer picture of containers on AWS\nFrom Docker, ECR, ECS, EKS to App Runner, each service fits a different level of complexity:\nApp Runner for “fast deployment with minimal infrastructure management”. ECS (EC2/Fargate) for moderate microservices that are “AWS-native”. EKS when full Kubernetes capabilities are required. Monitoring \u0026amp; Observability are mandatory, not “nice-to-have”\nCloudWatch is not just a log viewer; it is the central hub for metrics, logs, alarms, and dashboards. X-Ray expands observability into the “trace” dimension, providing insight into the journey of each request in a microservices environment. Overall, the event on 17/11 helped me connect previously scattered DevOps pieces into a complete picture on the AWS platform.\nIV. Application roadmap for studies and personal projects Standardize the DevOps workflow for current projects\nApply trunk-based development to personal/internship repositories.\nSet up a sample pipeline with CodeCommit → CodeBuild → CodeDeploy → CodePipeline, in which:\nUnit tests run automatically. Docker images are automatically built and pushed to ECR. Gradually phase out ClickOps and move to IaC\nFor current sandbox/lab environments, I will start by describing infrastructure with CloudFormation (VPC, EC2, S3, IAM Role). Once familiar, I will move to AWS CDK for larger projects and reuse constructs for VPC, clusters, RDS, etc. Incorporate containers into my learning pipeline and products\nPackage AI/ML applications and personal web projects into Docker images. Try deploying one service on ECS Fargate and another simple service on App Runner to compare cost and operational complexity. Set up minimal Monitoring \u0026amp; Observability for every workload\nEnable CloudWatch metrics \u0026amp; logs for EC2, Lambda, and ECS tasks. Create at least one basic dashboard and alarm (CPU, 5xx errors). For critical APIs, integrate X-Ray to practice distributed tracing. Career and certification orientation\nBased on the event content, I consider DevOps/Platform Engineer a suitable path to combine infrastructure knowledge and programming skills. In the medium term, I aim to prepare for AWS Certified Developer – Associate, and then move towards AWS DevOps Engineer – Professional once I have sufficient hands-on experience. This reflection serves as a foundation for systematizing my knowledge of DevOps on AWS and as a concrete guideline for designing and optimizing systems and personal projects in the upcoming period.\nV. Some photos from the event "
},
{
	"uri": "//localhost:1313/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: Vietnam Cloud Day 2025\nDate \u0026amp; Time: 13:00, Thursday, 18 September 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Summary: The Vietnam Cloud Day 2025 event (GenAI and Data Track) focused on the Agentic AI ecosystem and data platforms for GenAI on AWS. Speakers presented AWS’s Agentic AI vision with Amazon Bedrock, Amazon Nova, Strands Agents, and Bedrock AgentCore, while emphasizing the importance of a unified data foundation (S3, Lake House, Redshift, Iceberg, SageMaker, DataZone) to support analytics and AI at scale. The content also covered the GenAI roadmap \u0026amp; AI-DLC (AI-Driven Development Lifecycle) with three phases: Inception – Construction – Operation; security and Responsible AI principles (Well-Architected, MITRE ATLAS, OWASP LLM, NIST, ISO, EU AI Act); along with productivity tools such as Amazon Q, QuickSight, and specialized AI Agents. Through the event, I gained a clearer understanding of how AWS shapes Agentic AI architectures from data and models to applications; the crucial role of a data foundation before deploying GenAI; the importance of security, risk governance, and compliance; as well as how to apply AI-DLC and services like Bedrock, Amazon Q, Guardrails, and QuickSight to build GenAI systems and AI Agents for enterprises.\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 – 16:30, 03/10/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Summary: The “Reimagining Software Engineering” event focused on rethinking the entire software development life cycle in the GenAI era through the AI-Driven Development Lifecycle (AI-DLC) model and tools such as Amazon Q Developer and KIRO – AI IDE. Toan Huynh’s talk clarified the evolution from auto-complete → assistant → agents, distinguished between the AI-assisted and AI-managed approaches, and introduced AI-DLC as a “balance point” where AI deeply supports planning, architecture, requirements clarification, scaffolding, refactoring, and testing, while developers still retain ownership of expertise, decision-making, and verification. Concepts such as mob development, spec-driven development, clearly defining context–input–output, requiring AI to record its reasoning, always starting by asking AI to create a plan, breaking tasks down, and separating sessions were emphasized as best practices when working with GenAI. My Nguyen’s part introduced KIRO as an AI IDE designed for the spec → prototype → production flow, supporting spec-driven development directly in the IDE, managing context, tracking workflows and integrating AI-DLC (via steering folders, workflow .md, internal rules), while remaining compatible with VSCode and Claude models. From this event, I learned to look at SDLC from an AI-driven perspective: AI can automate a large portion of activities, but humans must design the process, control quality, and bear ultimate responsibility; the effective way to leverage AI is to clearly define context, input/output requirements, capture reasoning, and use AI where structure is strongest (specs, test flows, refactoring), while choosing the right tools: Amazon Q Developer for end-to-end SDLC automation and KIRO for projects that require a fast, visual prototype-to-production flow.\nEvent 3 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 – 12:00, 15/11/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Summary: The event focused on the overall landscape of AI/ML and Generative AI on AWS, introducing service layers from AI Services to Amazon SageMaker and Amazon Bedrock. The main content revolved around foundation models (Claude, Llama, Titan), prompt engineering techniques (zero-shot, few-shot, chain-of-thought), and RAG architectures for building Q\u0026amp;A systems on internal enterprise data. Through this session, I gained a clearer understanding of the differences between traditional ML and GenAI, the role of SageMaker in standardizing end-to-end pipelines, and how to combine Bedrock + embeddings + RAG to reduce hallucinations and fully leverage internal data. This is a direct foundation for my ongoing ideas to build specialized chatbots (e.g., legal chatbots, document analysis).\nEvent 4 Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30 – 17:00, 17/11/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Summary: The DevOps on AWS event moved from the DevOps mindset to the tooling ecosystem and operational architecture on AWS: CodeCommit, CodeBuild, CodeDeploy, CodePipeline for CI/CD; CloudFormation and CDK for Infrastructure as Code; ECR, ECS, EKS, App Runner for containers; and CloudWatch, X-Ray for monitoring \u0026amp; observability. The content was presented as a logical chain: from automated build–test–deploy processes, infrastructure defined as code, to how microservices are deployed and monitored in real-world environments. Through this session, I learned to view DevOps as a holistic mindset (culture + process + tooling), understood clearly the benefits of IaC over ClickOps, grasped container options for different levels of complexity, and recognized that observability is mandatory once systems start to scale. This knowledge helps me define more clearly how to design CI/CD pipelines and deployment environments for my AI/ML and web application projects.\nEvent 5 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30 – 12:00, 29/11/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Summary: The event focused entirely on the Security Pillar in the AWS Well-Architected Framework with its 5 components: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. The content spanned from modern IAM architecture (multi-account, SSO, SCPs, permission boundaries, MFA, Access Analyzer) to detection-as-code with CloudTrail, GuardDuty, Security Hub; infrastructure security with VPC segmentation, Security Groups, NACLs, Network Firewall; encryption and key/secret management with KMS and Secrets Manager; and finally IR playbooks for situations such as compromised IAM keys, S3 public exposure, and EC2 malware. From this session, I better understood that security must be designed at the architectural level rather than being a “patch” at the end, learned how to build a minimal IAM and logging/detection foundation for every workload, and gained a more concrete view of preparing playbooks and automation for incident response. This is an important basis for designing AI/ML systems and applications on AWS that are secure, compliant, and resilient in the face of incidents.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.5-event5/",
	"title": "AWS Cloud Mastery Series #3",
	"tags": [],
	"description": "",
	"content": "AWS Well-Architected Security Pillar Monday, 29/11/2025, from 8:30 – 17:00 – AWS Vietnam Office I. General information about the event Event name: AWS Well-Architected Security Pillar\nTime: 08:30 – 12:00, 29/11/2025 (morning only)\nVenue: AWS Vietnam Office\nMain objectives:\nClarify the role of the Security Pillar within the AWS Well-Architected Framework and the Shared Responsibility model. Systematize the 5 security pillars: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, Incident Response. Clarify best practices and common “pitfalls” for enterprises, especially in the context of cloud environments in Vietnam. Provide a practical perspective through small demos (IAM policy simulation, detection-as-code, network firewall, IR playbook). II. Detailed content by timeline 2.1. Opening \u0026amp; Security Foundation (08:30 – 08:50) The opening session focused on the overall picture of security on AWS:\nRole of the Security Pillar in Well-Architected\nSecurity was presented as a mandatory foundation, not a “nice-to-have” add-on at the end. If you design an architecture while ignoring the Security Pillar, the system is highly likely to encounter situations such as:\nSecurity breach (data leakage, exposure of sensitive information). System outages caused by misconfiguration, DDoS attacks, or operational errors. Data corruption or data loss due to lack of backup/encryption. Incidents under high traffic without appropriate protection and monitoring mechanisms. Core principles:\nLeast Privilege: grant only the exact minimum permissions required. Zero Trust: do not implicitly trust any component, even within the VPC. Defense in Depth: protect at multiple layers (identity, network, application, data, etc.). Shared Responsibility Model:\nAWS is responsible for “security OF the cloud” (physical infrastructure, availability zones, hypervisor, etc.). Customers are responsible for “security IN the cloud” (IAM, service configuration, source code, data, logging, IR, etc.). Top threats in the Vietnamese cloud environment:\nEmphasis on common risks:\nExposure/misuse of long-lived credentials. Unintentionally public S3 buckets. Workloads unnecessarily exposed to the public Internet. Lack of logging/monitoring leading to late incident detection. This section established the mindset that security must be designed from the outset, based on the 5 pillars and shared responsibility, not treated as a “firefighting” exercise after incidents occur.\n2.2. Pillar 1 – Identity \u0026amp; Access Management (08:50 – 09:30) – Modern IAM Architecture This session focused on modernizing IAM towards multi-account, SSO, centralized governance:\nIAM: Users, Roles, Policies – avoiding long-term credentials\nEmphasis: do not use long-lived access keys attached to IAM users for workloads; instead use IAM Roles + temporary credentials. Apply the principle of least privilege in every policy; avoid overusing \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;. IAM Identity Center \u0026amp; AWS Organizations\nIAM Identity Center was introduced as an SSO solution to centrally manage access across multiple accounts and applications. Enabling SSO typically goes together with AWS Organizations, so it is necessary to clearly design the organizational structure + account structure from the beginning (prod / non-prod / security / logging, etc.). SCPs – Service Control Policies\nThe speaker compared SCPs to “prohibition signs”, whereas IAM policies are the “driver’s license”:\nIAM policies grant permissions. SCPs define the maximum ceiling of permissions that can be granted within an account/OU. Principle: SCPs do not grant permissions; they only filter what IAM is allowed to grant.\nPermission Boundaries\nDescribed as an “advanced IAM” tool for handling complex delegation use cases:\nAllow teams to create IAM roles/users themselves while still being constrained by a “maximum permission boundary”. MFA, Credential Rotation, IAM Access Analyzer\nMFA: comparison between TOTP and FIDO2, with a recommendation to enable MFA for the root account and critical identities. Credential rotation: use AWS Secrets Manager to automatically rotate secrets, avoiding static secrets. IAM Access Analyzer: analyzes policies/conditions to detect unintended access (public, cross-account, etc.) and sends alerts when risks are found. Mini Demo: Validate IAM Policy + simulate access\nDemo illustrated using tools to validate IAM policies and “simulate” what an identity is effectively allowed to do on a resource, helping avoid misconfiguration before going to production. 2.3. Pillar 2 – Detection \u0026amp; Continuous Monitoring (09:30 – 09:55) This section, presented by Đức Anh, Tuấn Thịnh, Thanh Đạt, focused on detection \u0026amp; continuous monitoring:\nAWS CloudTrail (Đức Anh)\nDescribed as the “backbone” of detection on AWS:\nLogs and centrally monitors all API calls across all accounts.\nProvides multi-layer security visibility:\nManagement events (configuration changes). Data events (S3 object access, Lambda invoke, etc.). Network activity events. Organization-wide coverage when enabled at the organization level. Integrated with EventBridge to:\nReceive real-time events. Automatically send notifications or trigger workflows (Lambda, SNS, SQS, Step Functions). Support the detection-as-code concept: detection rules are also managed as code. Amazon GuardDuty (Tuấn Thịnh)\nFocused on threat detection based on:\nVPC Flow Logs, CloudTrail, DNS logs, and various other sources. Notes mentioned Advanced Protection Plans:\nAdditional protection for S3, EKS, RDS, Lambda, runtime monitoring. GuardDuty can integrate with EventBridge to automate response (send notifications, isolate instances, etc.) and also fits the detection-as-code model.\nAWS Security Hub CSPM (Thanh Đạt)\nAddresses the challenge: many services, many accounts, many compliance standards → difficult to manage manually.\nKey points:\nStandardizes outputs into ASFF (AWS Security Finding Format). Detects misconfigurations, assesses security posture. Applies standards such as AWS Foundational Security Best Practices, CIS Foundations Benchmark. Supports the detection-as-code model with centralized control for multi-account, multi-region environments. 2.4. Pillar 3 – Infrastructure Protection (10:10 – 10:40) – Network \u0026amp; Workload Security This section, presented by Kha, revolved around network and workload security:\nCommon Network Attack Vectors \u0026amp; Use Cases\nAnalyzed common attack vectors: inbound, outbound, east–west (between VPCs/subnets/workloads). AWS Layered Security \u0026amp; VPC Controls\nSecurity Groups (SG):\nStateful: when inbound is allowed, the corresponding outbound is automatically allowed. No “deny” rules, only allow rules. Supports security group sharing (RAM) and security group referencing across VPCs/accounts. Network ACLs (NACLs):\nAttached at the subnet layer, typically used for coarse-grained policies. Supports both allow and deny rules. Drawback: limited rule count and relatively simple inspection. DNS \u0026amp; Perimeter Protection\nAmazon Route 53: mentioned in its role as DNS:\nPrivate DNS, VPC DNS, Public DNS. Supports DNS filtering, centralized rule and reporting management, suitable for both cloud-only and hybrid network models. AWS Network Firewall\nFocus on:\nEgress filtering, environment segmentation, intrusion prevention. Supports both stateless and stateful rules, integrates with Transit Gateway, multi-VPC endpoints to control traffic across many VPCs.\nIn addition, the agenda emphasized AWS WAF + AWS Shield as the web application and DDoS protection layers, placed at the edge (ALB/CloudFront) in a defense-in-depth model.\n2.5. Pillar 4 – Data Protection (10:40 – 11:10) – Encryption, Keys \u0026amp; Secrets This section, presented by Thịnh Lâm and Việt Nguyễn, focused on encryption and key/secret management:\nAWS KMS (Key Management Service)\nClarified the concepts of master key and data key, and how data keys are generated and encrypted by master keys. Key policy: only principals explicitly defined in the policy can use the key; even administrators cannot use it if they are not included. Distinguished between managed keys and CMKs (customer-managed keys); CMKs can be configured for automatic or manual rotation. Data Classification \u0026amp; Guardrails\nNotes mentioned Amazon Macie to scan S3 buckets and identify sensitive data (PII, critical documents). Establish guardrails: for example, policies that deny access if S3 objects are not encrypted, enforcing encryption for sensitive data. Encryption in Transit\nList of services to consider:\nS3 \u0026amp; DynamoDB at the API level. RDS (SSL), EBS \u0026amp; Nitro (encryption on the wire), ACM (issuing TLS certificates). Secrets Manager \u0026amp; Rotation\nUse Secrets Manager to store credentials, connection strings, API keys, etc. Supports automatic rotation, integrates with KMS to encrypt secrets. 2.6. Pillar 5 – Incident Response (11:10 – 11:40) – IR Playbook \u0026amp; Automation This section, shared by Long and Tịnh Trương, focused on incident response (IR) processes:\nContext:\nModern environments (multi-account, many services, hybrid) are too complex to “firefight manually”. Types of incidents: security breaches, system outages, data corruption, high traffic, etc., require structured processes and a high degree of automation. Security Responsibilities are Shared\nRevisited the Shared Responsibility Model and listed key foundational services that should be enabled for security:\nAWS Organizations + SCP CloudTrail AWS Config GuardDuty Security Hub Prevention Guidelines (prevention over cure)\nKill long-lived credentials – eliminate long-term access keys. Never expose S3 – do not public S3 unless truly necessary and strictly controlled. No facing internet for workloads that do not need public access. Everything through IaC – all infrastructure changes should go through code. Double gate high-risk changes – add extra approval layers for high-risk changes. Incident Response Process (according to AWS)\nThe standard IR process includes:\nPrepare – build playbooks, assign roles, prepare tools in advance. Detection \u0026amp; Analysis – detect via GuardDuty/Security Hub/CloudTrail, analyze impact. Containment – isolate the source of the incident (isolate instance, revoke keys, etc.). Eradication \u0026amp; Recovery – remove the root cause, restore systems from snapshots/backups. Post-incident – review, extract lessons learned, update playbooks. Specific playbooks mentioned in the agenda:\nCompromised IAM key. S3 public exposure. EC2 malware detection. Common steps include: snapshotting, isolation, evidence collection, and automation using Lambda/Step Functions for recurring scenarios. 2.7. Wrap-Up \u0026amp; Q\u0026amp;A (11:40 – 12:00) The wrap-up emphasized:\nThe 5 Security Pillars must be seen as a cohesive framework, not separate elements.\nCommon pitfalls of Vietnamese enterprises:\nLack of standardized IAM (frequent use of root, long-lived credentials). Not enabling CloudTrail/GuardDuty/Security Hub comprehensively. Unintentionally public S3 or endpoints without guardrails. Suggested learning roadmap: AWS Certified Security – Specialty, then advanced Security topics within the Solutions Architect Professional track.\nIII. Knowledge and lessons learned Security must be architected from day one, not patched after incidents\nThe continuous reference to the Security Pillar in the Well-Architected Framework reinforced the idea that if the architecture does not enforce IAM, logging, network segmentation, encryption, etc. from the beginning, the cost of remediation later will be very high. Modern IAM = multi-account, SSO, SCP, permission boundaries, rotation, MFA\nIAM is not just “creating users and attaching policies”; it is a multi-layer architecture: Organizations, IAM Identity Center, SCPs, permission boundaries, Access Analyzer, etc., combined to deliver both flexibility and security. Detection-as-Code is essential in complex cloud environments\nInstead of manually inspecting logs, all detection rules (CloudTrail, GuardDuty, Security Hub) should be described as code, versioned, and deployed via pipelines – similar to how we manage IaC. Infrastructure security is not just “closing ports” but designing the network correctly\nClearly distinguishing the roles of SGs vs NACLs, and the roles of Route 53, Network Firewall, etc., shows that security must be tightly coupled with multi-layer network architecture. Data Protection and IR are the final links but critically important\nIt is not enough to encrypt data (KMS, encryption in transit); we must also classify data (Macie), enforce guardrails that deny non-encrypted resources, and manage secrets properly. Without pre-prepared IR playbooks, teams will become “overwhelmed” when real incidents occur. IV. Application roadmap for studies and personal projects Standardize IAM \u0026amp; multi-account for lab/personal project environments\nApply a minimal model of AWS Organizations + IAM Identity Center + SCP, avoiding putting everything into a single account. Gradually remove long-lived credentials, replace them with roles + temporary credentials, and enable MFA for critical accounts. Establish a detection foundation for every workload\nEnable CloudTrail at the org level, activate GuardDuty and Security Hub across all personal lab accounts, at least at a baseline level. Experiment with some detection-as-code rules (for example: S3 public access, security groups open to 0.0.0.0/0, credentials not rotated, etc.). Redesign VPC/network towards defense-in-depth\nSplit VPCs into clear public/private subnets; minimize workloads directly exposed to the Internet. Use security groups with meaningful semantics (by service role), NACLs at the subnet level, and consider experimenting with Network Firewall/Route 53 DNS filtering in advanced labs. Apply Data Protection \u0026amp; IR to projects with sensitive data\nFor projects involving contracts and user data, plan to use KMS, S3 encryption, RDS encryption, Secrets Manager, and Macie.\nBuild a simple IR playbook for lab environments:\nScenario where an access key is exposed. Scenario where an S3 bucket becomes public. Experiment with Lambda/Step Functions to automatically isolate resources or revoke credentials in certain simulated scenarios.\nPlan to pursue Security Specialty\nThe event reaffirmed that this is an area in which I need to invest seriously if I want to build AI/ML systems and enterprise applications on AWS in a secure and sustainable way. In my roadmap, I intend to:\nFurther consolidate knowledge of the Well-Architected Security Pillar and AWS SRA (Security Reference Architecture). Then aim for AWS Certified Security – Specialty as a medium-term goal. This reflection helps me synthesize the entire content of the 29/11 session according to the 5 Security Pillar components and clearly shape how to apply them to real-world systems that I am building or will build on AWS.\nV. Some photos from the event "
},
{
	"uri": "//localhost:1313/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "//localhost:1313/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "//localhost:1313/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "//localhost:1313/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "Over the 12 weeks participating in the First Cloud Journey program at Amazon Web Services (AWS) Vietnam as an FCJ Cloud Intern, I had the opportunity to experience a complete, structured “cloud onboarding” journey similar to a full-time employee: from learning AWS fundamentals and building the infrastructure foundation, to practicing application modernization and applying generative AI to real-world use cases. The program began with weeks focused on getting familiar with the working environment, regulations, team culture, and internal training systems; in parallel, I was guided to create and configure an AWS Free Tier account, install and use the AWS CLI, get used to the AWS Management Console, and explore core service groups such as Compute, Storage, Networking, and Database. Through basic hands-on labs with Amazon EC2 (creating instances, SSH access, managing EBS, Elastic IP), I built a solid foundation in how AWS organizes resources, handles access control, and operates infrastructure on the cloud.\nOn that foundation, the following weeks sequentially focused on designing standardized infrastructure architecture (VPC, subnets, routes, security groups, private connectivity), managing cost and security following the AWS Well-Architected guidelines; then gradually transitioned to application modernization with serverless architecture (AWS Lambda, Amazon API Gateway, Amazon DynamoDB, S3, Cognito) and experimenting with AI/ML services—especially Amazon Bedrock with models like Claude for use cases such as internal assistants, RAG, and business support scenarios. In parallel with the technical work, I maintained a detailed weekly Worklog, wrote proposals, translated and summarized various AWS technical blogs, and participated in workshops and events such as the AWS Cloud Mastery Series. These activities helped me improve my ability to read and understand English documentation, systematize knowledge, and present technical content to others.\nIn terms of soft skills and workplace experience, I worked in a very friendly and open environment, receiving close support from mentors and the admin team; the working style, which emphasizes “trying things yourself and solving problems independently,” helped me mature in both technical thinking and professional attitude. The assigned tasks were closely aligned with my specialization in Artificial Intelligence and Cloud Computing, while also expanding into practical skills such as using task management tools, teamwork, communication in a corporate environment, and technical writing. After 12 weeks, I not only strengthened my understanding of AWS core services and how to design and operate systems on the cloud, but also gained a clearer view of workflows, culture, and expectations inside a global technology organization like AWS Vietnam, forming a solid foundation for my future career in the Cloud \u0026amp; AI domain.\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding of the field, ability to apply knowledge in practice, tool proficiency, work quality ✅ ☐ ☐ 2 Ability to learn Absorbing new knowledge, learning quickly ☐ ✅ ☐ 3 Proactiveness Self-driven learning, taking on tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to working hours, rules, and processes ☐ ✅ ☐ 6 Drive for improvement Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in team activities ✅ ☐ ☐ 9 Professional conduct Showing respect to colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving mindset Identifying problems, proposing solutions, and being creative ☐ ✅ ☐ 11 Contribution to project/organization Work effectiveness, improvement initiatives, and recognition from the team ✅ ☐ ☐ 12 Overall Overall evaluation of the entire internship ✅ ☐ ☐ Areas for Improvement Improve problem-solving mindset, adaptability, and learning capability. Learn to communicate better in a professional working environment, especially in terms of soft skills. "
},
{
	"uri": "//localhost:1313/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working environment The working environment is very friendly and open. Members in FCJ are always willing to help when I face difficulties, even outside working hours. The workspace is tidy and comfortable, which helps me stay focused.\n2. Support from mentor / admin team The mentor provides very detailed guidance, explains clearly whenever I don’t understand something, and always encourages me to ask questions.\n3. Alignment between work and field of study The tasks I was assigned are aligned with the knowledge I learned at school, while also expanding into new areas I had never been exposed to before. Thanks to that, I was able to both reinforce my foundational knowledge and learn practical skills related to DevOps and Cloud.\n4. Learning opportunities \u0026amp; skill development During the internship, I learned many new skills such as using project management tools, teamwork skills, and how to communicate professionally in a corporate environment. The mentor also shared a lot of real-world experience that helped me better shape my career direction.\n5. Culture \u0026amp; team spirit The company culture is very positive: everyone respects each other, works seriously but still maintains a cheerful atmosphere. When there are urgent projects, everyone puts in effort together and supports one another regardless of position. This makes me feel like I am part of the team, even though I am only an intern.\n6. Policies / benefits for interns The company provides internship allowance and offers flexibility in working hours when necessary. In addition, being able to join internal training sessions is a major plus.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]